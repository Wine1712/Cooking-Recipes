{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f012667",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a66edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & Utilities ---\n",
    "import os                    # File and directory operations\n",
    "import re                    # Regular expressions for string cleaning\n",
    "import string                # String operations\n",
    "import ast                   # Safe evaluation of Python expressions\n",
    "import random                # Random operations (shuffling, sampling)\n",
    "import time                  # Time tracking for performance\n",
    "from collections import Counter  # Count frequencies of tokens\n",
    "\n",
    "# --- Data Handling ---\n",
    "import pandas as pd          # DataFrames for structured data\n",
    "import numpy as np           # Numerical operations and arrays\n",
    "\n",
    "# --- Natural Language Processing (NLP) ---\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize           # Tokenize sentences/words\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.corpus import stopwords, wordnet        # Common stopwords and WordNet\n",
    "from nltk.stem import WordNetLemmatizer           # Lemmatization for word normalization\n",
    "from bert_score import score as bert_score        # Semantic similarity metric\n",
    "\n",
    "# --- Deep Learning with PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- Visualization & Progress Tracking ---\n",
    "import matplotlib.pyplot as plt                   # Plotting graphs\n",
    "from tqdm import tqdm                              # Progress bar for loops\n",
    "\n",
    "# --- Word Embeddings ---\n",
    "from gensim.models import KeyedVectors            # Load pretrained word vectors\n",
    "\n",
    "# --- Memory Management ---\n",
    "import gc                                          # Garbage collection to manage RAM\n",
    "\n",
    "# --- Optimization ---\n",
    "import torch.optim as optim                       # Optimizers (e.g., Adam, SGD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168da80c",
   "metadata": {},
   "source": [
    "#### Set up the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df3b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Device Configuration ---\n",
    "# Automatically select the best available device: MPS (Apple), CUDA (GPU), or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"✅ Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429480b",
   "metadata": {},
   "source": [
    "#### Special Tokens and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Special Tokens & Model Hyperparameters ---\n",
    "# Define tokens for padding, start/end of sequence, and unknown words\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# Set model hyperparameters and data preprocessing constants\n",
    "MAX_INGREDIENT_LEN = 20     # Max number of input tokens\n",
    "MAX_RECIPE_LEN = 60         # Max number of output tokens\n",
    "BATCH_SIZE = 64             # Training batch size\n",
    "EMB_DIM = 64                # Word embedding dimension\n",
    "HIDDEN_DIM = 256            # RNN hidden state dimension\n",
    "TEACHER_FORCING_RATIO = 0.8 # Probability of using teacher forcing during training\n",
    "num_iters = 10000           # Total training iterations\n",
    "print_every = 50            # Frequency of logging training loss\n",
    "plot_every = 50             # Frequency of computing validation loss and plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868afa1",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ce9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSVs\n",
    "train_df = pd.read_csv('/Users/myatpwintphyu/Desktop/Monash/2025 S1/NLP/Assignment_2/Cooking_Dataset/train.csv')\n",
    "dev_df = pd.read_csv('/Users/myatpwintphyu/Desktop/Monash/2025 S1/NLP/Assignment_2/Cooking_Dataset/dev.csv')\n",
    "test_df = pd.read_csv('/Users/myatpwintphyu/Desktop/Monash/2025 S1/NLP/Assignment_2/Cooking_Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NLP Preprocessing Utilities ---\n",
    "lemmatizer = WordNetLemmatizer()                        # Word lemmatizer for reducing words to base form\n",
    "stop_words = set(stopwords.words('english'))            # English stopwords to remove from text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Cleaning and Formatting Functions ---\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes a text string by lowercasing, removing non-alphabetic characters,\n",
    "    removing stopwords, and applying lemmatization.\n",
    "    \"\"\"\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', str(text).lower())\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return words if words else [\"unknown\"]\n",
    "\n",
    "def preprocess_ingredients_column(df):\n",
    "    \"\"\"Adds 'input_tokens' column by preprocessing the 'Ingredients' field.\"\"\"\n",
    "    df['input_tokens'] = df['Ingredients'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "def preprocess_recipes_column(df):\n",
    "    \"\"\"Adds 'output_tokens' column by preprocessing the 'Recipe' field.\"\"\"\n",
    "    df['output_tokens'] = df['Recipe'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "def format_input_prompt(tokens):\n",
    "    \"\"\"Formats ingredient tokens as a prompt string for generation.\"\"\"\n",
    "    return f\"Generate recipe for: {', '.join(tokens)}\"\n",
    "\n",
    "def format_target_text(row):\n",
    "    \"\"\"Formats a row into a readable multi-line recipe string.\"\"\"\n",
    "    title = row['Title'] if 'Title' in row else 'Generated Recipe'\n",
    "    ingredients = ', '.join(row['input_tokens'])\n",
    "    instructions = ', '.join(row['output_tokens'])\n",
    "    return f\"Title: {title}\\n\\nIngredients:\\n{ingredients}\\nInstructions:\\n{instructions}\"\n",
    "\n",
    "def build_vocab(token_lists):\n",
    "    \"\"\"\n",
    "    Builds vocabulary from token lists and assigns unique index to each token.\n",
    "    Includes special tokens.\n",
    "    \"\"\"\n",
    "    vocab = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
    "    counter = Counter(token for tokens in token_lists for token in tokens)\n",
    "    for token in counter:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff993b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply Preprocessing to Dataset Splits ---\n",
    "# Tokenize and clean text fields for each split (train/dev/test)\n",
    "train_df = preprocess_ingredients_column(train_df)\n",
    "train_df = preprocess_recipes_column(train_df)\n",
    "\n",
    "dev_df = preprocess_ingredients_column(dev_df)\n",
    "dev_df = preprocess_recipes_column(dev_df)\n",
    "\n",
    "test_df = preprocess_ingredients_column(test_df)\n",
    "test_df = preprocess_recipes_column(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6748177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Input Prompts for Model ---\n",
    "# Converts token lists into natural language prompts\n",
    "train_df['input_prompt'] = train_df['input_tokens'].apply(format_input_prompt)\n",
    "dev_df['input_prompt'] = dev_df['input_tokens'].apply(format_input_prompt)\n",
    "test_df['input_prompt'] = test_df['input_tokens'].apply(format_input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Readable Target Texts ---\n",
    "# Combine title, ingredients, and instructions into formatted strings\n",
    "train_df['target_text'] = train_df.apply(format_target_text, axis=1)\n",
    "dev_df['target_text'] = dev_df.apply(format_target_text, axis=1)\n",
    "test_df['target_text'] = test_df.apply(format_target_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1074372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build Vocabularies ---\n",
    "# Construct token-to-index mappings from training data\n",
    "input_vocab = build_vocab(train_df['input_tokens'])\n",
    "output_vocab = build_vocab(train_df['output_tokens'])\n",
    "\n",
    "# Create reverse index-to-token dictionaries for decoding\n",
    "input_idx2word = {i: w for w, i in input_vocab.items()}\n",
    "output_idx2word = {i: w for w, i in output_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c314658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tokens, vocab, max_len, is_target=False):\n",
    "    \"\"\"\n",
    "    Encodes a list of tokens into a fixed-length list of token IDs using a vocabulary.\n",
    "\n",
    "    Args:\n",
    "        tokens: List of tokens (strings) to encode.\n",
    "        vocab: Dictionary mapping tokens to indices.\n",
    "        max_len: Maximum allowed length of the output sequence.\n",
    "        is_target: If True, adds <sos> at the start and <eos> at the end.\n",
    "\n",
    "    Returns:\n",
    "        A list of token IDs (integers), padded or truncated to max_len.\n",
    "    \"\"\"\n",
    "    encoded = []  # Initialize the list of token IDs\n",
    "\n",
    "    if is_target:\n",
    "        # Add <sos> token at the beginning if it's a target sequence\n",
    "        encoded.append(vocab[SOS_TOKEN])\n",
    "    \n",
    "    for tok in tokens:\n",
    "        # Convert each token to its corresponding index in the vocab\n",
    "        # Use <unk> token index if token is not found\n",
    "        encoded.append(vocab.get(tok, vocab[UNK_TOKEN]))\n",
    "\n",
    "    if is_target:\n",
    "        # Add <eos> token at the end if it's a target sequence\n",
    "        encoded.append(vocab[EOS_TOKEN])\n",
    "\n",
    "    # Ensure the sequence is exactly max_len in length\n",
    "    if len(encoded) > max_len:\n",
    "        # Truncate if too long\n",
    "        encoded = encoded[:max_len]\n",
    "    else:\n",
    "        # Pad with <pad> tokens if too short\n",
    "        encoded += [vocab[PAD_TOKEN]] * (max_len - len(encoded))\n",
    "\n",
    "    return encoded  # Return the fixed-length list of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8907583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CookingDataset(Dataset):\n",
    "    def __init__(self, df, input_vocab, output_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): The dataset containing tokenized ingredients and recipes.\n",
    "            input_vocab (dict): Vocabulary mapping for input tokens (ingredients).\n",
    "            output_vocab (dict): Vocabulary mapping for output tokens (recipes).\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (rows) in the DataFrame\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get token lists from the DataFrame at the given index\n",
    "        src_tokens = self.df.iloc[idx]['input_tokens']   # Ingredients\n",
    "        trg_tokens = self.df.iloc[idx]['output_tokens']  # Recipe steps\n",
    "\n",
    "        # Encode the tokens using the vocabularies\n",
    "        src_encoded = encode_text(src_tokens, self.input_vocab, MAX_INGREDIENT_LEN)\n",
    "        trg_encoded = encode_text(trg_tokens, self.output_vocab, MAX_RECIPE_LEN, is_target=True)\n",
    "\n",
    "        # Return as PyTorch tensors\n",
    "        return torch.tensor(src_encoded), torch.tensor(trg_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d27d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader to pad sequences in a batch.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of (input_tensor, target_tensor) tuples from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        src_padded: Padded input tensor of shape (batch_size, max_input_len).\n",
    "        trg_padded: Padded target tensor of shape (batch_size, max_target_len).\n",
    "    \"\"\"\n",
    "    # Unpack the batch into two lists: inputs and targets\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "\n",
    "    # Pad input sequences to the length of the longest in the batch\n",
    "    # batch_first=True -> shape will be (batch_size, seq_len)\n",
    "    src_padded = pad_sequence(\n",
    "        src_batch, batch_first=True, padding_value=input_vocab[PAD_TOKEN]\n",
    "    )\n",
    "\n",
    "    # Pad target sequences similarly\n",
    "    trg_padded = pad_sequence(\n",
    "        trg_batch, batch_first=True, padding_value=output_vocab[PAD_TOKEN]\n",
    "    )\n",
    "\n",
    "    # Return padded input and target batches\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb03747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CookingDataset(train_df, input_vocab, output_vocab)\n",
    "dev_dataset = CookingDataset(dev_df, input_vocab, output_vocab)\n",
    "test_dataset = CookingDataset(test_df, input_vocab, output_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307b721",
   "metadata": {},
   "source": [
    "#### Epoch Time Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4418ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format time nicely\n",
    "def epoch_time(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Calculates elapsed time between two time points and formats it into minutes and seconds.\n",
    "\n",
    "    Args:\n",
    "        start_time: Float timestamp at the start (e.g., from time.time()).\n",
    "        end_time: Float timestamp at the end.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (minutes, seconds) representing the duration.\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes = int(elapsed_time / 60)\n",
    "    seconds = int(elapsed_time - (minutes * 60))\n",
    "    return minutes, seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b7aa4",
   "metadata": {},
   "source": [
    "#### Evaluating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdb8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line creates a smoothing function for the BLEU score calculation using NLTK’s SmoothingFunction.\n",
    "smoothie = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a443d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, input_vocab, output_vocab, device, model_name=\"Seq2Seq-RNN\"):\n",
    "    \"\"\"\n",
    "    Evaluates a trained Seq2Seq model using BLEU, METEOR, and BERTScore.\n",
    "\n",
    "    Args:\n",
    "        model: The trained sequence-to-sequence model.\n",
    "        data_loader: A DataLoader for the validation or test set.\n",
    "        input_vocab: Vocabulary mapping for input tokens (not used in this function).\n",
    "        output_vocab: Vocabulary mapping for output tokens (used for decoding).\n",
    "        device: PyTorch device (e.g., 'cuda' or 'cpu').\n",
    "        model_name: A string label for the model (used in the results dictionary).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing average BLEU, METEOR, and BERTScore values for the dataset.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store evaluation metrics and text outputs\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Create a reverse mapping from index to word for decoding output tokens\n",
    "    output_idx2word = {i: w for w, i in output_vocab.items()}\n",
    "\n",
    "    # Disable gradient calculation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop through batches from the validation/test DataLoader\n",
    "        for src_batch, trg_batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move batches to the correct device (CPU or GPU)\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "\n",
    "            # Get model predictions without teacher forcing\n",
    "            output = model(src_batch, trg_batch, teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Get the most likely token index at each time step\n",
    "            # Shape: (batch_size, seq_len)\n",
    "            output_ids = output.argmax(2)\n",
    "\n",
    "            # Loop through each prediction-reference pair in the batch\n",
    "            for pred_seq, true_seq in zip(output_ids, trg_batch):\n",
    "                # Convert predicted token IDs to words, removing special tokens\n",
    "                pred_tokens = [\n",
    "                    output_idx2word.get(idx.item(), UNK_TOKEN)\n",
    "                    for idx in pred_seq\n",
    "                    if idx.item() not in [output_vocab[PAD_TOKEN], output_vocab[SOS_TOKEN], output_vocab[EOS_TOKEN]]\n",
    "                ]\n",
    "\n",
    "                true_tokens = [\n",
    "                    output_idx2word.get(idx.item(), UNK_TOKEN)\n",
    "                    for idx in true_seq\n",
    "                    if idx.item() not in [output_vocab[PAD_TOKEN], output_vocab[SOS_TOKEN], output_vocab[EOS_TOKEN]]\n",
    "                ]\n",
    "\n",
    "                # Store string versions for corpus-level evaluation later\n",
    "                predictions.append(\" \".join(pred_tokens))\n",
    "                references.append(\" \".join(true_tokens))\n",
    "\n",
    "                # Compute individual BLEU and METEOR scores for this sample\n",
    "                bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "                meteor = meteor_score([true_tokens], pred_tokens)\n",
    "\n",
    "                bleu_scores.append(bleu)\n",
    "                meteor_scores.append(meteor)\n",
    "\n",
    "    # Compute BERTScore once for the entire corpus\n",
    "    try:\n",
    "        P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n",
    "        bert_f1 = F1.mean().item()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ BERTScore skipped due to error: {e}\")\n",
    "        bert_f1 = None\n",
    "\n",
    "    # Compute average scores across all samples\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"METEOR\": sum(meteor_scores) / len(meteor_scores),\n",
    "        \"BERTScore\": bert_f1 if bert_f1 is not None else \"Unavailable\"\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8f820",
   "metadata": {},
   "source": [
    "#### Evaluation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef0bb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, val_loader, criterion, output_vocab, device):\n",
    "    \"\"\"\n",
    "    Computes the average loss of a trained model on a validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Seq2Seq model.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function (e.g., nn.CrossEntropyLoss).\n",
    "        output_vocab: Target vocabulary dictionary (not used directly here).\n",
    "        device: PyTorch device (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        Average validation loss across all batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src_batch, trg_batch in val_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "\n",
    "            output = model(src_batch, trg_batch, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            target = trg_batch[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef344f7",
   "metadata": {},
   "source": [
    "#### Training Loop for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, output_vocab, device,\n",
    "                num_iters, print_every, plot_every, teacher_forcing_ratio=0.5, \n",
    "                log_filename=\"training_log.csv\", plot_title=\"Training and Validation Loss\"):\n",
    "    \"\"\"\n",
    "    Trains a Seq2Seq model using random batches from the training data.\n",
    "    Also tracks validation loss and logs progress over time.\n",
    "    \"\"\"\n",
    "    \n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    print_loss_total = 0  # Running total for printing\n",
    "    plot_loss_total = 0   # Running total for plotting\n",
    "    first_val_loss = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Cache all training batches to randomly sample from them\n",
    "    train_batches = list(train_loader)\n",
    "\n",
    "    for iteration in range(1, num_iters + 1):\n",
    "        # === Randomly sample a batch from training set ===\n",
    "        input_batch, target_batch = random.choice(train_batches)\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # === Forward pass ===\n",
    "        output = model(input_batch, target_batch, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "        # === Reshape output and target for loss computation ===\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)      # Skip <sos>\n",
    "        target = target_batch[:, 1:].reshape(-1)\n",
    "\n",
    "        # === Compute loss and backpropagation ===\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        print_loss_total += loss_value\n",
    "        plot_loss_total += loss_value\n",
    "\n",
    "        # === Print training progress every 'print_every' iterations ===\n",
    "        if iteration % print_every == 0:\n",
    "            avg_loss = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"[Iter {iteration:05d}] ⏱ Time: {elapsed:.1f}s | Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # === Validate and track performance every 'plot_every' iterations ===\n",
    "        if iteration % plot_every == 0:\n",
    "            val_loss_total = 0\n",
    "            model.eval()  # Set model to evaluation mode (e.g., disable dropout)\n",
    "            with torch.no_grad():  # Disable gradient tracking for validation\n",
    "                for val_input, val_target in val_loader:\n",
    "                    val_input = val_input.to(device)\n",
    "                    val_target = val_target.to(device)\n",
    "\n",
    "                    # In validation, use greedy decoding (no teacher forcing)\n",
    "                    val_output = model(val_input, val_target, teacher_forcing_ratio=0.0)\n",
    "                    val_output = val_output[:, 1:].reshape(-1, output_dim)\n",
    "                    val_target = val_target[:, 1:].reshape(-1)\n",
    "                    val_loss = criterion(val_output, val_target)\n",
    "                    val_loss_total += val_loss.item()\n",
    "\n",
    "            # Compute and store average validation loss\n",
    "            avg_val_loss = val_loss_total / len(val_loader)\n",
    "            train_losses.append(plot_loss_total / plot_every)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print(f\"📉 Iter {iteration}: Val Loss = {avg_val_loss:.4f}\")\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    # === Save training and validation loss log to CSV ===\n",
    "    log_data = {\n",
    "        \"Iteration\": list(range(plot_every, plot_every * len(train_losses) + 1, plot_every)),\n",
    "        \"Training Loss\": train_losses,\n",
    "        \"Validation Loss\": val_losses\n",
    "    }\n",
    "    log_df = pd.DataFrame(log_data)\n",
    "    log_df.to_csv(log_filename, index=False)\n",
    "    print(f\"📁 Training log saved to {log_filename}\")\n",
    "\n",
    "    # === Plot training vs. validation loss ===\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(log_data[\"Iteration\"], train_losses, label='Training Loss')\n",
    "    plt.plot(log_data[\"Iteration\"], val_losses, label='Validation Loss')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae420b",
   "metadata": {},
   "source": [
    "#### Generating the Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c34cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, src_tensor, trg_vocab, device, max_len=MAX_RECIPE_LEN, use_attention=False):\n",
    "    \"\"\"\n",
    "    Generates a recipe from a single input using either attention-based or vanilla seq2seq.\n",
    "\n",
    "    Args:\n",
    "        model: full seq2seq model or tuple (encoder, decoder)\n",
    "        src_tensor: tensor of shape (src_len,)\n",
    "        trg_vocab: target vocabulary (word to index)\n",
    "        device: torch device (cpu/cuda)\n",
    "        max_len: max length of the generated recipe\n",
    "        use_attention: set to True if using (encoder, decoder) attention-style models\n",
    "\n",
    "    Returns:\n",
    "        String of generated recipe text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    trg_vocab_inv = {i: w for w, i in trg_vocab.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension: (1, src_len)\n",
    "        src_tensor = src_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        if use_attention:\n",
    "            # Expecting (encoder, decoder) as a tuple\n",
    "            encoder, decoder = model\n",
    "            encoder_outputs, hidden = encoder(src_tensor)\n",
    "\n",
    "            input_token = torch.tensor([trg_vocab[SOS_TOKEN]], device=device)\n",
    "            result = []\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                output, hidden, _ = decoder(input_token, hidden, encoder_outputs)\n",
    "                top1 = output.argmax(1).item()\n",
    "\n",
    "                if top1 == trg_vocab[EOS_TOKEN]:\n",
    "                    break\n",
    "                result.append(trg_vocab_inv.get(top1, UNK_TOKEN))\n",
    "                input_token = torch.tensor([top1], device=device)\n",
    "\n",
    "        else:\n",
    "            # Expecting model to be Seq2Seq class with internal encoder+decoder\n",
    "            dummy_trg = torch.zeros((1, max_len), dtype=torch.long, device=device)\n",
    "            output = model(src_tensor, dummy_trg, teacher_forcing_ratio=0.0)\n",
    "            pred_ids = output.argmax(2).squeeze(0).tolist()\n",
    "\n",
    "            result = []\n",
    "            for idx in pred_ids:\n",
    "                token = trg_vocab_inv.get(idx, UNK_TOKEN)\n",
    "                if token == EOS_TOKEN:\n",
    "                    break\n",
    "                if token not in [PAD_TOKEN, SOS_TOKEN]:\n",
    "                    result.append(token)\n",
    "\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb9de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        RNN-based encoder for a sequence-to-sequence model.\n",
    "\n",
    "        This module takes an input sequence of token indices, embeds them, and passes the\n",
    "        embeddings through a RNN to produce hidden state(s) used by the decoder.\n",
    "\n",
    "        Args:\n",
    "            input_vocab_size: Size of the input vocabulary.\n",
    "            emb_dim: Dimensionality of the token embeddings.\n",
    "            hidden_dim: Dimensionality of the RNN hidden state.\n",
    "\n",
    "        Methods:\n",
    "            forward(src): Encodes an input tensor and returns RNN outputs and final hidden state.\n",
    "            initHidden(): Initializes a zero hidden state tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_vocab_size, emb_dim, hidden_dim):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        # Embedding layer to convert token indices to dense vectors\n",
    "        self.embedding = nn.Embedding(input_vocab_size, emb_dim)\n",
    "\n",
    "        # RNN layer to process the sequence of embeddings\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Dropout layer for regularization (helps prevent overfitting)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Store the hidden state dimension\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: input sequence tensor of shape (batch_size, seq_len)\n",
    "\n",
    "        embedded = self.embedding(src)          # Convert tokens to embeddings: (batch_size, seq_len, emb_dim)\n",
    "        output, hidden = self.rnn(embedded)     # Pass through RNN: output (batch, seq_len, hidden_dim), hidden (1, batch, hidden_dim)\n",
    "        output = self.dropout(output)           # Apply dropout to the RNN output\n",
    "\n",
    "        return output, hidden                   # Return both the full sequence output and final hidden state\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Create initial hidden state filled with zeros (for single batch/time)\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e78039b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-based decoder for a sequence-to-sequence model.\n",
    "\n",
    "    This module receives the previous token (or start token), encodes it,\n",
    "    and generates the next token in the sequence, one step at a time.\n",
    "\n",
    "    Args:\n",
    "        output_vocab_size: Size of the output vocabulary.\n",
    "        emb_dim: Dimensionality of the token embeddings.\n",
    "        hidden_dim: Dimensionality of the RNN hidden state.\n",
    "\n",
    "    Methods:\n",
    "        forward(input_token, hidden): Generates the next token output and updated hidden state.\n",
    "        initHidden(batch_size, device): Returns an initial hidden state of zeros.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_vocab_size, emb_dim, hidden_dim):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Embedding layer to convert token indices to embeddings\n",
    "        self.embedding = nn.Embedding(output_vocab_size, emb_dim)\n",
    "\n",
    "        # RNN layer to process embedded tokens one step at a time\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Dropout layer to reduce overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layer to project RNN output to vocabulary space\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_vocab_size)\n",
    "\n",
    "        # Store hidden dimension size\n",
    "        self.hidden_size = hidden_dim  # ✅ fixed typo\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        # input_token: (batch,) — one token per sequence in the batch\n",
    "\n",
    "        # Embed the input token and add a time dimension: (batch, 1, emb_dim)\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)\n",
    "\n",
    "        # Pass through RNN using the previous hidden state\n",
    "        # output: (batch, 1, hidden_dim), hidden: (1, batch, hidden_dim)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "\n",
    "        # Apply dropout to RNN output\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # Remove the time dimension and pass through the output layer to get predictions\n",
    "        # prediction: (batch, vocab_size)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "\n",
    "        return prediction, hidden  # Return predicted token scores and new hidden state\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        # Initialize the hidden state with zeros: (1, batch, hidden_dim)\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c10474dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "        Sequence-to-sequence model using RNN-based encoder and decoder.\n",
    "\n",
    "        This model takes an input sequence, encodes it into a hidden state,\n",
    "        and then generates the output sequence token by token, optionally using\n",
    "        teacher forcing during training.\n",
    "\n",
    "        Args:\n",
    "            encoder: Encoder module (e.g., EncoderRNN).\n",
    "            decoder: Decoder module (e.g., DecoderRNN).\n",
    "            device: PyTorch device ('cuda' or 'cpu').\n",
    "\n",
    "        Methods:\n",
    "            forward(src, trg, teacher_forcing_ratio):\n",
    "                Performs a forward pass through the model.\n",
    "                Returns predicted token distributions for the target sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        # Store encoder and decoder modules\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # Store device (CPU or GPU)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass for sequence-to-sequence model.\n",
    "\n",
    "        Args:\n",
    "            src: input sequence tensor of shape (batch, src_len)\n",
    "            trg: target sequence tensor of shape (batch, trg_len)\n",
    "            teacher_forcing_ratio: probability of using teacher forcing at each time step\n",
    "\n",
    "        Returns:\n",
    "            outputs: tensor of shape (batch, trg_len, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = trg.shape[0]           # Number of sequences in the batch\n",
    "        trg_len = trg.shape[1]              # Maximum target sequence length\n",
    "        vocab_size = self.decoder.fc_out.out_features  # Size of target vocabulary\n",
    "\n",
    "        # Initialize tensor to store decoder outputs for each time step\n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(self.device)\n",
    "\n",
    "        # Pass input through encoder (we discard encoder output and only use final hidden state)\n",
    "        _, hidden = self.encoder(src)  # hidden: (1, batch, hidden_dim)\n",
    "\n",
    "        # First input to decoder is the <sos> token from target\n",
    "        input_token = trg[:, 0]  # Shape: (batch,)\n",
    "\n",
    "        # Decode each time step in the target sequence (excluding first token)\n",
    "        for t in range(1, trg_len):\n",
    "            # Pass current input token and hidden state through decoder\n",
    "            output, hidden = self.decoder(input_token, hidden)  # output: (batch, vocab_size)\n",
    "\n",
    "            # Store decoder's output prediction at current time step\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            # Decide whether to use teacher forcing for next input\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "\n",
    "            # Get the most likely predicted token from the decoder's output\n",
    "            top1 = output.argmax(1)  # Shape: (batch,)\n",
    "\n",
    "            # If using teacher forcing, use actual target token as next input\n",
    "            # Otherwise, use model's own prediction\n",
    "            input_token = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        # Return all decoder outputs for the full sequence\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb619c",
   "metadata": {},
   "source": [
    "## The 2 Mild Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e2491",
   "metadata": {},
   "source": [
    "### Advanced Data Preprocessing and Baseline 1 — Sequence-to-Sequence Model without Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c146d34",
   "metadata": {},
   "source": [
    "#### Advanced Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a801d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unit Patterns ===\n",
    "UNITS = ['c', 'c.', 'cup', 'cups', 'tbsp', 'tbsp.', 'tablespoon', 'tablespoons', 'tsp', 'tsp.', 'teaspoon', 'teaspoons',\n",
    "    'oz', 'oz.', 'ounce', 'ounces', 'lb', 'lb.', 'pound', 'pounds', 'g', 'gram', 'grams', 'kg', 'kilogram',\n",
    "    'ml', 'ml.', 'l', 'liter', 'pkg', 'can', 'jar', 'box', 'container', 'bottle', 'stick', 'sticks', 'slice',\n",
    "    'slices', 'head', 'bunch', 'carton', 'dash', 'pinch', 'sq', 'sq.', 'envelope', 'bag', 'sheet']\n",
    "unit_pattern = re.compile(r'\\b(' + '|'.join(UNITS) + r')\\b', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b5d2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    \"\"\"\n",
    "    Converts input text to lowercase.\n",
    "    Args:\n",
    "        text: A string.\n",
    "    Returns:\n",
    "        Lowercased string.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input text.\n",
    "    Args:\n",
    "        text: A string with space-separated words.\n",
    "    Returns:\n",
    "        A string with stopwords removed.\n",
    "    \"\"\"\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    Applies lemmatization to each word in the input text.\n",
    "    Args:\n",
    "        text: A string with space-separated words.\n",
    "    Returns:\n",
    "        A string with lemmatized words.\n",
    "    \"\"\"\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "def clean_ingredient(text):\n",
    "    \"\"\"\n",
    "    Cleans a single ingredient string by:\n",
    "    - Lowercasing\n",
    "    - Replacing numbers with <num>\n",
    "    - Removing measurement units and punctuation\n",
    "    - Removing stopwords\n",
    "    - Lemmatizing\n",
    "\n",
    "    Args:\n",
    "        text: A raw ingredient string.\n",
    "    Returns:\n",
    "        A cleaned string representing the ingredient.\n",
    "    \"\"\"\n",
    "    text = lowercase(text)\n",
    "    text = re.sub(r'\\d+\\/\\d+|\\d+\\.\\d+|\\d+', '<num>', text)\n",
    "    text = unit_pattern.sub('', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_ingredient_list(ingredient_list):\n",
    "    \"\"\"\n",
    "    Cleans a list of ingredient strings from a stringified Python list.\n",
    "    Args:\n",
    "        ingredient_list: A stringified list of ingredients.\n",
    "    Returns:\n",
    "        A list of cleaned ingredient strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ingredients = ast.literal_eval(ingredient_list)\n",
    "        return [clean_ingredient(item) for item in ingredients]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def pad_recipes(batch_tokens, pad_token_id, max_len):\n",
    "    padded = []\n",
    "    for tokens in batch_tokens:\n",
    "        tokens = tokens.tolist()  # Convert tensor to list\n",
    "        tokens = tokens[:max_len] + [pad_token_id] * (max_len - len(tokens))\n",
    "        padded.append(tokens)\n",
    "    return torch.tensor(padded)\n",
    "\n",
    "\n",
    "def prune_vocab(token_lists, min_freq=2):\n",
    "    \"\"\"\n",
    "    Prunes a vocabulary by keeping only tokens with at least `min_freq` occurrences.\n",
    "    Args:\n",
    "        token_lists: A list of token lists.\n",
    "        min_freq: Minimum frequency threshold for retaining a token.\n",
    "    Returns:\n",
    "        A set of tokens that meet the frequency requirement.\n",
    "    \"\"\"\n",
    "    flat_tokens = [token for tokens in token_lists for token in tokens]\n",
    "    token_counts = Counter(flat_tokens)\n",
    "    return {token for token, count in token_counts.items() if count >= min_freq}\n",
    "\n",
    "\n",
    "def preprocess_ingredients_column_Mild_Extensions_1(df):\n",
    "    \"\"\"\n",
    "    Applies full ingredient cleaning pipeline to the Ingredients column\n",
    "    and stores results as tokenized list under 'input_tokens'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['input_tokens'] = df['Ingredients'].apply(clean_ingredient_list)\n",
    "    return df\n",
    "\n",
    "def preprocess_recipe_column_Mild_Extensions_1(df):\n",
    "    \"\"\"\n",
    "    Applies full ingredient cleaning pipeline to the Ingredients column\n",
    "    and stores results as tokenized list under 'input_tokens'.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['input_tokens'] = df['Recipe'].apply(clean_ingredient_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d47e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_Mild_Extensions_1(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader.\n",
    "    Pads input and target sequences in each batch to fixed lengths.\n",
    "    \"\"\"\n",
    "\n",
    "    # === Separate input and target sequences from the batch ===\n",
    "    input_seqs = [item[0] for item in batch]   # List of ingredient sequences\n",
    "    target_seqs = [item[1] for item in batch]  # List of recipe sequences\n",
    "\n",
    "    # === Pad input sequences (ingredients) ===\n",
    "    # Ensures all input sequences have the same length (MAX_INGREDIENT_LEN)\n",
    "    input_padded = pad_recipes(\n",
    "        input_seqs,\n",
    "        pad_token_id=input_vocab[PAD_TOKEN],  # Use PAD token for padding\n",
    "        max_len=MAX_INGREDIENT_LEN\n",
    "    )\n",
    "\n",
    "    # === Pad target sequences (recipes) ===\n",
    "    # Ensures all target sequences have the same length (MAX_RECIPE_LEN)\n",
    "    target_padded = pad_recipes(\n",
    "        target_seqs,\n",
    "        pad_token_id=output_vocab[PAD_TOKEN],\n",
    "        max_len=MAX_RECIPE_LEN\n",
    "    )\n",
    "\n",
    "    # === Return padded batches ===\n",
    "    return input_padded, target_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a00e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_Mild_Extensions_1 = preprocess_ingredients_column_Mild_Extensions_1(train_df)\n",
    "train_df_Mild_Extensions_1 = preprocess_recipe_column_Mild_Extensions_1(train_df)\n",
    "dev_df_Mild_Extensions_1 = preprocess_ingredients_column_Mild_Extensions_1(dev_df)\n",
    "dev_df_Mild_Extensions_1 = preprocess_recipe_column_Mild_Extensions_1(dev_df)\n",
    "test_df_Mild_Extensions_1 = preprocess_ingredients_column_Mild_Extensions_1(test_df)\n",
    "test_df_Mild_Extensions_1 = preprocess_recipe_column_Mild_Extensions_1(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b00cd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_input_tokens_train_df = prune_vocab(train_df_Mild_Extensions_1['input_tokens'], min_freq=2)\n",
    "# Apply pruning to all splits\n",
    "train_df_Mild_Extensions_1['input_tokens'] = train_df_Mild_Extensions_1['input_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t in pruned_input_tokens_train_df]\n",
    ")\n",
    "pruned_output_tokens_train_df = prune_vocab(train_df_Mild_Extensions_1['output_tokens'], min_freq=2)\n",
    "# Apply pruning to all splits\n",
    "train_df_Mild_Extensions_1['output_tokens'] = train_df_Mild_Extensions_1['output_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t in pruned_output_tokens_train_df]\n",
    ")\n",
    "\n",
    "dev_df_Mild_Extensions_1['input_tokens'] = dev_df_Mild_Extensions_1['input_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t in pruned_input_tokens_train_df]\n",
    ")\n",
    "\n",
    "test_df_Mild_Extensions_1['input_tokens'] = test_df_Mild_Extensions_1['input_tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if t in pruned_input_tokens_train_df]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Format input prompts and target texts ===\n",
    "# Converts token lists into formatted strings suitable for model input/output\n",
    "\n",
    "# Apply formatting to input tokens to create input prompts\n",
    "train_df_Mild_Extensions_1['input_prompt'] = train_df_Mild_Extensions_1['input_tokens'].apply(format_input_prompt)\n",
    "dev_df_Mild_Extensions_1['input_prompt'] = dev_df_Mild_Extensions_1['input_tokens'].apply(format_input_prompt)\n",
    "test_df_Mild_Extensions_1['input_prompt'] = test_df_Mild_Extensions_1['input_tokens'].apply(format_input_prompt)\n",
    "\n",
    "# Apply formatting to create target recipe texts from tokens\n",
    "train_df_Mild_Extensions_1['target_text'] = train_df_Mild_Extensions_1.apply(format_target_text, axis=1)\n",
    "dev_df_Mild_Extensions_1['target_text'] = dev_df_Mild_Extensions_1.apply(format_target_text, axis=1)\n",
    "test_df_Mild_Extensions_1['target_text'] = test_df_Mild_Extensions_1.apply(format_target_text, axis=1)\n",
    "\n",
    "# === Build vocabularies from the training set ===\n",
    "# These will be used to convert tokens into indices for model input/output\n",
    "\n",
    "input_vocab_Mild_Extensions_1 = build_vocab(train_df_Mild_Extensions_1['input_tokens'])   # Ingredients vocab\n",
    "output_vocab_Mild_Extensions_1 = build_vocab(train_df_Mild_Extensions_1['output_tokens']) # Recipe vocab\n",
    "\n",
    "# Create reverse index-to-word dictionaries for decoding outputs later\n",
    "input_idx2word_Mild_Extensions_1 = {i: w for w, i in input_vocab_Mild_Extensions_1.items()}\n",
    "output_idx2word_Mild_Extensions_1 = {i: w for w, i in output_vocab_Mild_Extensions_1.items()}\n",
    "\n",
    "# === Create dataset objects ===\n",
    "# Wrap each DataFrame with the custom CookingDataset class\n",
    "\n",
    "train_dataset_Mild_Extensions_1 = CookingDataset(\n",
    "    train_df_Mild_Extensions_1,\n",
    "    input_vocab_Mild_Extensions_1,\n",
    "    output_vocab_Mild_Extensions_1\n",
    ")\n",
    "\n",
    "dev_dataset_Mild_Extensions_1 = CookingDataset(\n",
    "    dev_df_Mild_Extensions_1,\n",
    "    input_vocab_Mild_Extensions_1,\n",
    "    output_vocab_Mild_Extensions_1\n",
    ")\n",
    "\n",
    "test_dataset_Mild_Extensions_1 = CookingDataset(\n",
    "    test_df_Mild_Extensions_1,\n",
    "    input_vocab_Mild_Extensions_1,\n",
    "    output_vocab_Mild_Extensions_1\n",
    ")\n",
    "\n",
    "# === Create DataLoaders with custom collate function ===\n",
    "# The collate function pads sequences to fixed length so the model can process batches\n",
    "\n",
    "train_loader_Mild_Extensions_1 = DataLoader(\n",
    "    train_dataset_Mild_Extensions_1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_Mild_Extensions_1\n",
    ")\n",
    "\n",
    "dev_loader_Mild_Extensions_1 = DataLoader(\n",
    "    dev_dataset_Mild_Extensions_1,\n",
    "    batch_size=1,  # Evaluate one sample at a time for validation\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_Mild_Extensions_1\n",
    ")\n",
    "\n",
    "test_loader_Mild_Extensions_1 = DataLoader(\n",
    "    test_dataset_Mild_Extensions_1,\n",
    "    batch_size=1,  # Evaluate one sample at a time for testing\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_Mild_Extensions_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a333c1c",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c7f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 00050] ⏱ Time: 28.6s | Avg Train Loss: 9.3337\n",
      "📉 Iter 50: Val Loss = 9.2658\n",
      "[Iter 00100] ⏱ Time: 74.3s | Avg Train Loss: 9.0619\n",
      "📉 Iter 100: Val Loss = 8.2908\n",
      "[Iter 00150] ⏱ Time: 119.9s | Avg Train Loss: 7.7141\n",
      "📉 Iter 150: Val Loss = 6.9029\n",
      "[Iter 00200] ⏱ Time: 166.7s | Avg Train Loss: 6.7672\n",
      "📉 Iter 200: Val Loss = 6.3316\n",
      "[Iter 00250] ⏱ Time: 213.5s | Avg Train Loss: 6.4264\n",
      "📉 Iter 250: Val Loss = 6.1249\n",
      "[Iter 00300] ⏱ Time: 258.3s | Avg Train Loss: 6.3221\n",
      "📉 Iter 300: Val Loss = 6.0445\n",
      "[Iter 00350] ⏱ Time: 303.0s | Avg Train Loss: 6.2560\n",
      "📉 Iter 350: Val Loss = 6.0112\n",
      "[Iter 00400] ⏱ Time: 348.3s | Avg Train Loss: 6.2358\n",
      "📉 Iter 400: Val Loss = 5.9888\n",
      "[Iter 00450] ⏱ Time: 393.1s | Avg Train Loss: 6.2048\n",
      "📉 Iter 450: Val Loss = 5.9757\n",
      "[Iter 00500] ⏱ Time: 438.2s | Avg Train Loss: 6.2039\n",
      "📉 Iter 500: Val Loss = 5.9667\n",
      "[Iter 00550] ⏱ Time: 483.1s | Avg Train Loss: 6.1835\n",
      "📉 Iter 550: Val Loss = 5.9598\n",
      "[Iter 00600] ⏱ Time: 530.4s | Avg Train Loss: 6.1933\n",
      "📉 Iter 600: Val Loss = 5.9540\n",
      "[Iter 00650] ⏱ Time: 581.9s | Avg Train Loss: 6.1718\n",
      "📉 Iter 650: Val Loss = 5.9503\n",
      "[Iter 00700] ⏱ Time: 627.7s | Avg Train Loss: 6.1527\n",
      "📉 Iter 700: Val Loss = 5.9442\n",
      "[Iter 00750] ⏱ Time: 676.0s | Avg Train Loss: 6.1795\n",
      "📉 Iter 750: Val Loss = 5.9419\n",
      "[Iter 00800] ⏱ Time: 722.4s | Avg Train Loss: 6.1387\n",
      "📉 Iter 800: Val Loss = 5.9372\n",
      "[Iter 00850] ⏱ Time: 768.0s | Avg Train Loss: 6.1163\n",
      "📉 Iter 850: Val Loss = 5.9323\n",
      "[Iter 00900] ⏱ Time: 812.7s | Avg Train Loss: 6.1330\n",
      "📉 Iter 900: Val Loss = 5.9304\n",
      "[Iter 00950] ⏱ Time: 859.2s | Avg Train Loss: 6.1310\n",
      "📉 Iter 950: Val Loss = 5.9288\n",
      "[Iter 01000] ⏱ Time: 903.9s | Avg Train Loss: 6.1012\n",
      "📉 Iter 1000: Val Loss = 5.9257\n",
      "[Iter 01050] ⏱ Time: 948.7s | Avg Train Loss: 6.0931\n",
      "📉 Iter 1050: Val Loss = 5.9209\n",
      "[Iter 01100] ⏱ Time: 996.5s | Avg Train Loss: 6.0773\n",
      "📉 Iter 1100: Val Loss = 5.9155\n",
      "[Iter 01150] ⏱ Time: 1042.2s | Avg Train Loss: 6.0929\n",
      "📉 Iter 1150: Val Loss = 5.9135\n",
      "[Iter 01200] ⏱ Time: 1088.2s | Avg Train Loss: 6.0682\n",
      "📉 Iter 1200: Val Loss = 5.9061\n",
      "[Iter 01250] ⏱ Time: 1134.5s | Avg Train Loss: 6.0652\n",
      "📉 Iter 1250: Val Loss = 5.9052\n",
      "[Iter 01300] ⏱ Time: 1181.1s | Avg Train Loss: 6.0418\n",
      "📉 Iter 1300: Val Loss = 5.8994\n",
      "[Iter 01350] ⏱ Time: 1226.9s | Avg Train Loss: 6.0589\n",
      "📉 Iter 1350: Val Loss = 5.9006\n",
      "[Iter 01400] ⏱ Time: 1271.8s | Avg Train Loss: 6.0139\n",
      "📉 Iter 1400: Val Loss = 5.8921\n",
      "[Iter 01450] ⏱ Time: 1317.8s | Avg Train Loss: 6.0369\n",
      "📉 Iter 1450: Val Loss = 5.8966\n",
      "[Iter 01500] ⏱ Time: 1362.9s | Avg Train Loss: 6.0346\n",
      "📉 Iter 1500: Val Loss = 5.8975\n",
      "[Iter 01550] ⏱ Time: 1407.9s | Avg Train Loss: 6.0037\n",
      "📉 Iter 1550: Val Loss = 5.8902\n",
      "[Iter 01600] ⏱ Time: 1454.9s | Avg Train Loss: 5.9717\n",
      "📉 Iter 1600: Val Loss = 5.9036\n",
      "[Iter 01650] ⏱ Time: 1503.1s | Avg Train Loss: 5.9919\n",
      "📉 Iter 1650: Val Loss = 5.9089\n",
      "[Iter 01700] ⏱ Time: 1548.4s | Avg Train Loss: 5.9748\n",
      "📉 Iter 1700: Val Loss = 5.9113\n",
      "[Iter 01750] ⏱ Time: 1597.2s | Avg Train Loss: 5.9633\n",
      "📉 Iter 1750: Val Loss = 5.9146\n",
      "[Iter 01800] ⏱ Time: 1644.0s | Avg Train Loss: 5.9580\n",
      "📉 Iter 1800: Val Loss = 5.9126\n",
      "[Iter 01850] ⏱ Time: 1690.3s | Avg Train Loss: 5.9444\n",
      "📉 Iter 1850: Val Loss = 5.9066\n",
      "[Iter 01900] ⏱ Time: 1736.2s | Avg Train Loss: 5.9164\n",
      "📉 Iter 1900: Val Loss = 5.9034\n",
      "[Iter 01950] ⏱ Time: 1782.6s | Avg Train Loss: 5.9264\n",
      "📉 Iter 1950: Val Loss = 5.9049\n",
      "[Iter 02000] ⏱ Time: 1820.0s | Avg Train Loss: 5.9534\n",
      "📉 Iter 2000: Val Loss = 5.8917\n",
      "[Iter 02050] ⏱ Time: 1859.0s | Avg Train Loss: 5.8903\n",
      "📉 Iter 2050: Val Loss = 5.8841\n",
      "[Iter 02100] ⏱ Time: 1895.9s | Avg Train Loss: 5.9001\n",
      "📉 Iter 2100: Val Loss = 5.8812\n",
      "[Iter 02150] ⏱ Time: 1932.3s | Avg Train Loss: 5.8777\n",
      "📉 Iter 2150: Val Loss = 5.8738\n",
      "[Iter 02200] ⏱ Time: 1968.9s | Avg Train Loss: 5.8773\n",
      "📉 Iter 2200: Val Loss = 5.8968\n",
      "[Iter 02250] ⏱ Time: 2011.3s | Avg Train Loss: 5.8706\n",
      "📉 Iter 2250: Val Loss = 5.8888\n",
      "[Iter 02300] ⏱ Time: 2054.3s | Avg Train Loss: 5.8615\n",
      "📉 Iter 2300: Val Loss = 5.8764\n",
      "[Iter 02350] ⏱ Time: 2101.3s | Avg Train Loss: 5.8832\n",
      "📉 Iter 2350: Val Loss = 5.8708\n",
      "[Iter 02400] ⏱ Time: 2143.8s | Avg Train Loss: 5.8700\n",
      "📉 Iter 2400: Val Loss = 5.8866\n",
      "[Iter 02450] ⏱ Time: 2184.7s | Avg Train Loss: 5.8486\n",
      "📉 Iter 2450: Val Loss = 5.8778\n",
      "[Iter 02500] ⏱ Time: 2231.2s | Avg Train Loss: 5.8145\n",
      "📉 Iter 2500: Val Loss = 5.8784\n",
      "[Iter 02550] ⏱ Time: 2277.1s | Avg Train Loss: 5.8240\n",
      "📉 Iter 2550: Val Loss = 5.8657\n",
      "[Iter 02600] ⏱ Time: 2317.2s | Avg Train Loss: 5.8186\n",
      "📉 Iter 2600: Val Loss = 5.8763\n",
      "[Iter 02650] ⏱ Time: 2358.4s | Avg Train Loss: 5.8112\n",
      "📉 Iter 2650: Val Loss = 5.8826\n",
      "[Iter 02700] ⏱ Time: 2403.2s | Avg Train Loss: 5.8123\n",
      "📉 Iter 2700: Val Loss = 5.8850\n",
      "[Iter 02750] ⏱ Time: 2448.5s | Avg Train Loss: 5.8039\n",
      "📉 Iter 2750: Val Loss = 5.8880\n",
      "[Iter 02800] ⏱ Time: 2490.4s | Avg Train Loss: 5.7768\n",
      "📉 Iter 2800: Val Loss = 5.8853\n",
      "[Iter 02850] ⏱ Time: 2537.0s | Avg Train Loss: 5.7680\n",
      "📉 Iter 2850: Val Loss = 5.8687\n",
      "[Iter 02900] ⏱ Time: 2582.5s | Avg Train Loss: 5.7704\n",
      "📉 Iter 2900: Val Loss = 5.8639\n",
      "[Iter 02950] ⏱ Time: 2631.4s | Avg Train Loss: 5.7359\n",
      "📉 Iter 2950: Val Loss = 5.8540\n",
      "[Iter 03000] ⏱ Time: 2669.1s | Avg Train Loss: 5.7657\n",
      "📉 Iter 3000: Val Loss = 5.8595\n",
      "[Iter 03050] ⏱ Time: 2716.0s | Avg Train Loss: 5.7339\n",
      "📉 Iter 3050: Val Loss = 5.8711\n",
      "[Iter 03100] ⏱ Time: 2764.3s | Avg Train Loss: 5.7348\n",
      "📉 Iter 3100: Val Loss = 5.8915\n",
      "[Iter 03150] ⏱ Time: 2816.5s | Avg Train Loss: 5.7316\n",
      "📉 Iter 3150: Val Loss = 5.8866\n",
      "[Iter 03200] ⏱ Time: 2864.0s | Avg Train Loss: 5.6962\n",
      "📉 Iter 3200: Val Loss = 5.8864\n",
      "[Iter 03250] ⏱ Time: 2910.4s | Avg Train Loss: 5.7021\n",
      "📉 Iter 3250: Val Loss = 5.8973\n",
      "[Iter 03300] ⏱ Time: 2955.3s | Avg Train Loss: 5.6993\n",
      "📉 Iter 3300: Val Loss = 5.8966\n",
      "[Iter 03350] ⏱ Time: 3003.2s | Avg Train Loss: 5.6870\n",
      "📉 Iter 3350: Val Loss = 5.8616\n",
      "[Iter 03400] ⏱ Time: 3043.0s | Avg Train Loss: 5.7010\n",
      "📉 Iter 3400: Val Loss = 5.8643\n",
      "[Iter 03450] ⏱ Time: 3091.8s | Avg Train Loss: 5.6569\n",
      "📉 Iter 3450: Val Loss = 5.9088\n",
      "[Iter 03500] ⏱ Time: 3135.6s | Avg Train Loss: 5.6791\n",
      "📉 Iter 3500: Val Loss = 5.8701\n",
      "[Iter 03550] ⏱ Time: 3178.6s | Avg Train Loss: 5.6492\n",
      "📉 Iter 3550: Val Loss = 5.8709\n",
      "[Iter 03600] ⏱ Time: 3222.7s | Avg Train Loss: 5.6508\n",
      "📉 Iter 3600: Val Loss = 5.8862\n",
      "[Iter 03650] ⏱ Time: 3265.8s | Avg Train Loss: 5.6455\n",
      "📉 Iter 3650: Val Loss = 5.9119\n",
      "[Iter 03700] ⏱ Time: 3313.1s | Avg Train Loss: 5.6335\n",
      "📉 Iter 3700: Val Loss = 5.8819\n",
      "[Iter 03750] ⏱ Time: 3357.3s | Avg Train Loss: 5.6283\n",
      "📉 Iter 3750: Val Loss = 5.8858\n",
      "[Iter 03800] ⏱ Time: 3402.0s | Avg Train Loss: 5.6201\n",
      "📉 Iter 3800: Val Loss = 5.9108\n",
      "[Iter 03850] ⏱ Time: 3449.1s | Avg Train Loss: 5.6398\n",
      "📉 Iter 3850: Val Loss = 5.9220\n",
      "[Iter 03900] ⏱ Time: 3497.8s | Avg Train Loss: 5.6305\n",
      "📉 Iter 3900: Val Loss = 5.8770\n",
      "[Iter 03950] ⏱ Time: 3544.0s | Avg Train Loss: 5.6143\n",
      "📉 Iter 3950: Val Loss = 5.8787\n",
      "[Iter 04000] ⏱ Time: 3588.9s | Avg Train Loss: 5.5968\n",
      "📉 Iter 4000: Val Loss = 5.8972\n",
      "[Iter 04050] ⏱ Time: 3635.3s | Avg Train Loss: 5.5985\n",
      "📉 Iter 4050: Val Loss = 5.9029\n",
      "[Iter 04100] ⏱ Time: 3688.8s | Avg Train Loss: 5.5915\n",
      "📉 Iter 4100: Val Loss = 5.8964\n",
      "[Iter 04150] ⏱ Time: 3730.8s | Avg Train Loss: 5.5846\n",
      "📉 Iter 4150: Val Loss = 5.9011\n",
      "[Iter 04200] ⏱ Time: 3777.9s | Avg Train Loss: 5.5533\n",
      "📉 Iter 4200: Val Loss = 5.8921\n",
      "[Iter 04250] ⏱ Time: 3824.0s | Avg Train Loss: 5.5687\n",
      "📉 Iter 4250: Val Loss = 5.8950\n",
      "[Iter 04300] ⏱ Time: 3870.9s | Avg Train Loss: 5.5486\n",
      "📉 Iter 4300: Val Loss = 5.8884\n",
      "[Iter 04350] ⏱ Time: 3911.5s | Avg Train Loss: 5.5463\n",
      "📉 Iter 4350: Val Loss = 5.9059\n",
      "[Iter 04400] ⏱ Time: 3956.2s | Avg Train Loss: 5.5606\n",
      "📉 Iter 4400: Val Loss = 5.9003\n",
      "[Iter 04450] ⏱ Time: 4004.7s | Avg Train Loss: 5.5476\n",
      "📉 Iter 4450: Val Loss = 5.9033\n",
      "[Iter 04500] ⏱ Time: 4049.3s | Avg Train Loss: 5.5410\n",
      "📉 Iter 4500: Val Loss = 5.9059\n",
      "[Iter 04550] ⏱ Time: 4091.3s | Avg Train Loss: 5.5378\n",
      "📉 Iter 4550: Val Loss = 5.9138\n",
      "[Iter 04600] ⏱ Time: 4137.9s | Avg Train Loss: 5.5384\n",
      "📉 Iter 4600: Val Loss = 5.9271\n",
      "[Iter 04650] ⏱ Time: 4183.9s | Avg Train Loss: 5.5363\n",
      "📉 Iter 4650: Val Loss = 5.9523\n",
      "[Iter 04700] ⏱ Time: 4227.8s | Avg Train Loss: 5.5184\n",
      "📉 Iter 4700: Val Loss = 5.9415\n",
      "[Iter 04750] ⏱ Time: 4270.7s | Avg Train Loss: 5.4986\n",
      "📉 Iter 4750: Val Loss = 5.9439\n",
      "[Iter 04800] ⏱ Time: 4317.1s | Avg Train Loss: 5.5017\n",
      "📉 Iter 4800: Val Loss = 5.9538\n",
      "[Iter 04850] ⏱ Time: 4365.6s | Avg Train Loss: 5.5322\n",
      "📉 Iter 4850: Val Loss = 5.9470\n",
      "[Iter 04900] ⏱ Time: 4406.9s | Avg Train Loss: 5.4934\n",
      "📉 Iter 4900: Val Loss = 5.9578\n",
      "[Iter 04950] ⏱ Time: 4469.8s | Avg Train Loss: 5.5247\n",
      "📉 Iter 4950: Val Loss = 5.9500\n",
      "[Iter 05000] ⏱ Time: 4543.3s | Avg Train Loss: 5.5036\n",
      "📉 Iter 5000: Val Loss = 5.9525\n",
      "[Iter 05050] ⏱ Time: 4592.5s | Avg Train Loss: 5.4897\n",
      "📉 Iter 5050: Val Loss = 5.9609\n",
      "[Iter 05100] ⏱ Time: 4685.0s | Avg Train Loss: 5.4755\n",
      "📉 Iter 5100: Val Loss = 5.9599\n",
      "[Iter 05150] ⏱ Time: 4800.6s | Avg Train Loss: 5.4882\n",
      "📉 Iter 5150: Val Loss = 5.9738\n",
      "[Iter 05200] ⏱ Time: 4846.0s | Avg Train Loss: 5.4542\n",
      "📉 Iter 5200: Val Loss = 5.9655\n",
      "[Iter 05250] ⏱ Time: 4891.6s | Avg Train Loss: 5.4445\n",
      "📉 Iter 5250: Val Loss = 5.9715\n",
      "[Iter 05300] ⏱ Time: 4930.4s | Avg Train Loss: 5.4883\n",
      "📉 Iter 5300: Val Loss = 5.9713\n",
      "[Iter 05350] ⏱ Time: 4974.8s | Avg Train Loss: 5.4359\n",
      "📉 Iter 5350: Val Loss = 5.9661\n",
      "[Iter 05400] ⏱ Time: 5019.8s | Avg Train Loss: 5.4720\n",
      "📉 Iter 5400: Val Loss = 5.9683\n",
      "[Iter 05450] ⏱ Time: 5065.5s | Avg Train Loss: 5.4360\n",
      "📉 Iter 5450: Val Loss = 5.9781\n",
      "[Iter 05500] ⏱ Time: 5105.5s | Avg Train Loss: 5.4424\n",
      "📉 Iter 5500: Val Loss = 5.9693\n",
      "[Iter 05550] ⏱ Time: 5148.5s | Avg Train Loss: 5.4241\n",
      "📉 Iter 5550: Val Loss = 5.9758\n",
      "[Iter 05600] ⏱ Time: 5195.0s | Avg Train Loss: 5.4582\n",
      "📉 Iter 5600: Val Loss = 5.9742\n",
      "[Iter 05650] ⏱ Time: 5242.0s | Avg Train Loss: 5.4079\n",
      "📉 Iter 5650: Val Loss = 5.9791\n",
      "[Iter 05700] ⏱ Time: 5283.4s | Avg Train Loss: 5.4483\n",
      "📉 Iter 5700: Val Loss = 5.9904\n",
      "[Iter 05750] ⏱ Time: 5327.6s | Avg Train Loss: 5.4293\n",
      "📉 Iter 5750: Val Loss = 5.9912\n",
      "[Iter 05800] ⏱ Time: 5373.3s | Avg Train Loss: 5.4309\n",
      "📉 Iter 5800: Val Loss = 5.9907\n",
      "[Iter 05850] ⏱ Time: 5414.0s | Avg Train Loss: 5.4056\n",
      "📉 Iter 5850: Val Loss = 5.9979\n",
      "[Iter 05900] ⏱ Time: 5457.3s | Avg Train Loss: 5.3979\n",
      "📉 Iter 5900: Val Loss = 5.9970\n",
      "[Iter 05950] ⏱ Time: 5503.8s | Avg Train Loss: 5.3863\n",
      "📉 Iter 5950: Val Loss = 5.9965\n",
      "[Iter 06000] ⏱ Time: 5552.9s | Avg Train Loss: 5.4212\n",
      "📉 Iter 6000: Val Loss = 6.0144\n",
      "[Iter 06050] ⏱ Time: 5594.1s | Avg Train Loss: 5.4002\n",
      "📉 Iter 6050: Val Loss = 6.0105\n",
      "[Iter 06100] ⏱ Time: 5638.9s | Avg Train Loss: 5.3966\n",
      "📉 Iter 6100: Val Loss = 6.0049\n",
      "[Iter 06150] ⏱ Time: 5686.9s | Avg Train Loss: 5.3807\n",
      "📉 Iter 6150: Val Loss = 6.0103\n",
      "[Iter 06200] ⏱ Time: 5735.4s | Avg Train Loss: 5.3904\n",
      "📉 Iter 6200: Val Loss = 6.0260\n",
      "[Iter 06250] ⏱ Time: 5776.3s | Avg Train Loss: 5.3626\n",
      "📉 Iter 6250: Val Loss = 6.0148\n",
      "[Iter 06300] ⏱ Time: 5820.6s | Avg Train Loss: 5.3659\n",
      "📉 Iter 6300: Val Loss = 6.0121\n",
      "[Iter 06350] ⏱ Time: 5865.9s | Avg Train Loss: 5.3710\n",
      "📉 Iter 6350: Val Loss = 6.0294\n",
      "[Iter 06400] ⏱ Time: 5911.1s | Avg Train Loss: 5.3522\n",
      "📉 Iter 6400: Val Loss = 6.0224\n",
      "[Iter 06450] ⏱ Time: 5953.9s | Avg Train Loss: 5.3852\n",
      "📉 Iter 6450: Val Loss = 6.0144\n",
      "[Iter 06500] ⏱ Time: 6001.7s | Avg Train Loss: 5.3691\n",
      "📉 Iter 6500: Val Loss = 6.0183\n",
      "[Iter 06550] ⏱ Time: 6050.0s | Avg Train Loss: 5.3673\n",
      "📉 Iter 6550: Val Loss = 6.0118\n",
      "[Iter 06600] ⏱ Time: 6091.5s | Avg Train Loss: 5.3613\n",
      "📉 Iter 6600: Val Loss = 6.0243\n",
      "[Iter 06650] ⏱ Time: 6135.3s | Avg Train Loss: 5.3623\n",
      "📉 Iter 6650: Val Loss = 6.0104\n",
      "[Iter 06700] ⏱ Time: 6182.8s | Avg Train Loss: 5.3626\n",
      "📉 Iter 6700: Val Loss = 6.0241\n",
      "[Iter 06750] ⏱ Time: 6231.5s | Avg Train Loss: 5.3231\n",
      "📉 Iter 6750: Val Loss = 6.0460\n",
      "[Iter 06800] ⏱ Time: 6271.8s | Avg Train Loss: 5.3401\n",
      "📉 Iter 6800: Val Loss = 6.0523\n",
      "[Iter 06850] ⏱ Time: 6313.7s | Avg Train Loss: 5.3228\n",
      "📉 Iter 6850: Val Loss = 6.0337\n",
      "[Iter 06900] ⏱ Time: 6357.3s | Avg Train Loss: 5.3120\n",
      "📉 Iter 6900: Val Loss = 6.0369\n",
      "[Iter 06950] ⏱ Time: 6402.2s | Avg Train Loss: 5.3188\n",
      "📉 Iter 6950: Val Loss = 6.0462\n",
      "[Iter 07000] ⏱ Time: 6439.9s | Avg Train Loss: 5.3320\n",
      "📉 Iter 7000: Val Loss = 6.0375\n",
      "[Iter 07050] ⏱ Time: 6482.4s | Avg Train Loss: 5.3524\n",
      "📉 Iter 7050: Val Loss = 6.0451\n",
      "[Iter 07100] ⏱ Time: 6526.2s | Avg Train Loss: 5.2919\n",
      "📉 Iter 7100: Val Loss = 6.0552\n",
      "[Iter 07150] ⏱ Time: 6569.3s | Avg Train Loss: 5.2911\n",
      "📉 Iter 7150: Val Loss = 6.0452\n",
      "[Iter 07200] ⏱ Time: 6608.5s | Avg Train Loss: 5.3123\n",
      "📉 Iter 7200: Val Loss = 6.0391\n",
      "[Iter 07250] ⏱ Time: 6651.2s | Avg Train Loss: 5.3088\n",
      "📉 Iter 7250: Val Loss = 6.0693\n",
      "[Iter 07300] ⏱ Time: 6695.6s | Avg Train Loss: 5.2891\n",
      "📉 Iter 7300: Val Loss = 6.0625\n",
      "[Iter 07350] ⏱ Time: 6736.5s | Avg Train Loss: 5.2854\n",
      "📉 Iter 7350: Val Loss = 6.0453\n",
      "[Iter 07400] ⏱ Time: 6776.8s | Avg Train Loss: 5.2761\n",
      "📉 Iter 7400: Val Loss = 6.0647\n",
      "[Iter 07450] ⏱ Time: 6819.4s | Avg Train Loss: 5.2773\n",
      "📉 Iter 7450: Val Loss = 6.0715\n",
      "[Iter 07500] ⏱ Time: 6864.5s | Avg Train Loss: 5.3028\n",
      "📉 Iter 7500: Val Loss = 6.0540\n",
      "[Iter 07550] ⏱ Time: 6903.2s | Avg Train Loss: 5.2785\n",
      "📉 Iter 7550: Val Loss = 6.0595\n",
      "[Iter 07600] ⏱ Time: 6944.8s | Avg Train Loss: 5.2980\n",
      "📉 Iter 7600: Val Loss = 6.0499\n",
      "[Iter 07650] ⏱ Time: 6988.1s | Avg Train Loss: 5.2505\n",
      "📉 Iter 7650: Val Loss = 6.0519\n",
      "[Iter 07700] ⏱ Time: 7033.0s | Avg Train Loss: 5.3181\n",
      "📉 Iter 7700: Val Loss = 6.0762\n",
      "[Iter 07750] ⏱ Time: 7070.8s | Avg Train Loss: 5.2812\n",
      "📉 Iter 7750: Val Loss = 6.0711\n",
      "[Iter 07800] ⏱ Time: 7114.7s | Avg Train Loss: 5.2444\n",
      "📉 Iter 7800: Val Loss = 6.0640\n",
      "[Iter 07850] ⏱ Time: 7158.7s | Avg Train Loss: 5.2575\n",
      "📉 Iter 7850: Val Loss = 6.0802\n",
      "[Iter 07900] ⏱ Time: 7201.8s | Avg Train Loss: 5.2398\n",
      "📉 Iter 7900: Val Loss = 6.0770\n",
      "[Iter 07950] ⏱ Time: 7240.9s | Avg Train Loss: 5.2424\n",
      "📉 Iter 7950: Val Loss = 6.0857\n",
      "[Iter 08000] ⏱ Time: 7283.8s | Avg Train Loss: 5.2553\n",
      "📉 Iter 8000: Val Loss = 6.0775\n",
      "[Iter 08050] ⏱ Time: 7328.5s | Avg Train Loss: 5.2451\n",
      "📉 Iter 8050: Val Loss = 6.0874\n",
      "[Iter 08100] ⏱ Time: 7369.2s | Avg Train Loss: 5.2480\n",
      "📉 Iter 8100: Val Loss = 6.0928\n",
      "[Iter 08150] ⏱ Time: 7409.3s | Avg Train Loss: 5.2525\n",
      "📉 Iter 8150: Val Loss = 6.0681\n",
      "[Iter 08200] ⏱ Time: 7452.2s | Avg Train Loss: 5.2835\n",
      "📉 Iter 8200: Val Loss = 6.1086\n",
      "[Iter 08250] ⏱ Time: 7497.8s | Avg Train Loss: 5.2637\n",
      "📉 Iter 8250: Val Loss = 6.0897\n",
      "[Iter 08300] ⏱ Time: 7537.2s | Avg Train Loss: 5.2208\n",
      "📉 Iter 8300: Val Loss = 6.0902\n",
      "[Iter 08350] ⏱ Time: 7579.2s | Avg Train Loss: 5.2557\n",
      "📉 Iter 8350: Val Loss = 6.0969\n",
      "[Iter 08400] ⏱ Time: 7622.4s | Avg Train Loss: 5.2312\n",
      "📉 Iter 8400: Val Loss = 6.0928\n",
      "[Iter 08450] ⏱ Time: 7668.7s | Avg Train Loss: 5.2252\n",
      "📉 Iter 8450: Val Loss = 6.0963\n",
      "[Iter 08500] ⏱ Time: 7706.5s | Avg Train Loss: 5.2234\n",
      "📉 Iter 8500: Val Loss = 6.0899\n",
      "[Iter 08550] ⏱ Time: 7749.3s | Avg Train Loss: 5.2044\n",
      "📉 Iter 8550: Val Loss = 6.1086\n",
      "[Iter 08600] ⏱ Time: 7793.0s | Avg Train Loss: 5.2214\n",
      "📉 Iter 8600: Val Loss = 6.1049\n",
      "[Iter 08650] ⏱ Time: 7838.1s | Avg Train Loss: 5.1930\n",
      "📉 Iter 8650: Val Loss = 6.1077\n",
      "[Iter 08700] ⏱ Time: 7876.7s | Avg Train Loss: 5.2408\n",
      "📉 Iter 8700: Val Loss = 6.0988\n",
      "[Iter 08750] ⏱ Time: 7919.6s | Avg Train Loss: 5.2106\n",
      "📉 Iter 8750: Val Loss = 6.0881\n",
      "[Iter 08800] ⏱ Time: 7963.8s | Avg Train Loss: 5.2064\n",
      "📉 Iter 8800: Val Loss = 6.1138\n",
      "[Iter 08850] ⏱ Time: 8006.1s | Avg Train Loss: 5.1936\n",
      "📉 Iter 8850: Val Loss = 6.1337\n",
      "[Iter 08900] ⏱ Time: 8046.3s | Avg Train Loss: 5.2322\n",
      "📉 Iter 8900: Val Loss = 6.1069\n",
      "[Iter 08950] ⏱ Time: 8089.5s | Avg Train Loss: 5.1935\n",
      "📉 Iter 8950: Val Loss = 6.1010\n",
      "[Iter 09000] ⏱ Time: 8134.4s | Avg Train Loss: 5.1914\n",
      "📉 Iter 9000: Val Loss = 6.1130\n",
      "[Iter 09050] ⏱ Time: 8174.7s | Avg Train Loss: 5.1616\n",
      "📉 Iter 9050: Val Loss = 6.1074\n",
      "[Iter 09100] ⏱ Time: 8216.3s | Avg Train Loss: 5.2036\n",
      "📉 Iter 9100: Val Loss = 6.1163\n",
      "[Iter 09150] ⏱ Time: 8259.4s | Avg Train Loss: 5.2340\n",
      "📉 Iter 9150: Val Loss = 6.1278\n",
      "[Iter 09200] ⏱ Time: 8305.0s | Avg Train Loss: 5.2117\n",
      "📉 Iter 9200: Val Loss = 6.1202\n",
      "[Iter 09250] ⏱ Time: 8343.5s | Avg Train Loss: 5.1956\n",
      "📉 Iter 9250: Val Loss = 6.1411\n",
      "[Iter 09300] ⏱ Time: 8385.8s | Avg Train Loss: 5.1807\n",
      "📉 Iter 9300: Val Loss = 6.1328\n",
      "[Iter 09350] ⏱ Time: 8429.4s | Avg Train Loss: 5.1972\n",
      "📉 Iter 9350: Val Loss = 6.1220\n",
      "[Iter 09400] ⏱ Time: 8475.2s | Avg Train Loss: 5.1888\n",
      "📉 Iter 9400: Val Loss = 6.1446\n",
      "[Iter 09450] ⏱ Time: 8513.0s | Avg Train Loss: 5.2020\n",
      "📉 Iter 9450: Val Loss = 6.1295\n",
      "[Iter 09500] ⏱ Time: 8556.8s | Avg Train Loss: 5.1678\n",
      "📉 Iter 9500: Val Loss = 6.1407\n",
      "[Iter 09550] ⏱ Time: 8600.6s | Avg Train Loss: 5.1922\n",
      "📉 Iter 9550: Val Loss = 6.1621\n",
      "[Iter 09600] ⏱ Time: 8643.8s | Avg Train Loss: 5.2046\n",
      "📉 Iter 9600: Val Loss = 6.1273\n",
      "[Iter 09650] ⏱ Time: 8682.7s | Avg Train Loss: 5.1591\n",
      "📉 Iter 9650: Val Loss = 6.1532\n",
      "[Iter 09700] ⏱ Time: 8725.8s | Avg Train Loss: 5.1605\n",
      "📉 Iter 9700: Val Loss = 6.1308\n",
      "[Iter 09750] ⏱ Time: 8770.2s | Avg Train Loss: 5.1783\n",
      "📉 Iter 9750: Val Loss = 6.1559\n",
      "[Iter 09800] ⏱ Time: 8812.0s | Avg Train Loss: 5.1570\n",
      "📉 Iter 9800: Val Loss = 6.1458\n",
      "[Iter 09850] ⏱ Time: 8851.8s | Avg Train Loss: 5.1910\n",
      "📉 Iter 9850: Val Loss = 6.1611\n",
      "[Iter 09900] ⏱ Time: 8894.9s | Avg Train Loss: 5.1706\n",
      "📉 Iter 9900: Val Loss = 6.1872\n",
      "[Iter 09950] ⏱ Time: 8940.9s | Avg Train Loss: 5.1678\n",
      "📉 Iter 9950: Val Loss = 6.1672\n",
      "[Iter 10000] ⏱ Time: 8981.5s | Avg Train Loss: 5.1809\n",
      "📉 Iter 10000: Val Loss = 6.1488\n",
      "📁 Training log saved to Sequence-to-Sequence model without attention_Mild_Extensions_1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Instantiate Encoder and Decoder for Mild_Extensions_1 ===\n",
    "# Encoder processes the input (ingredients), and the Decoder generates the output (recipe)\n",
    "encoder_Mild_Extensions_1 = EncoderRNN(\n",
    "    input_vocab_size=len(input_vocab),   # Size of input vocabulary\n",
    "    emb_dim=EMB_DIM,                     # Embedding dimension\n",
    "    hidden_dim=HIDDEN_DIM                # Hidden layer size\n",
    ").to(device)\n",
    "\n",
    "decoder_Mild_Extensions_1 = DecoderRNN(\n",
    "    output_vocab_size=len(output_vocab), # Size of output vocabulary\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM\n",
    ").to(device)\n",
    "\n",
    "# === Define the baseline Seq2Seq model (without attention) ===\n",
    "model_Mild_Extensions_1 = Seq2Seq(\n",
    "    encoder=encoder_Mild_Extensions_1,\n",
    "    decoder=decoder_Mild_Extensions_1,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# === Define optimizer and loss function ===\n",
    "# Adam optimizer with small learning rate; loss function ignores <PAD> tokens\n",
    "optimizer_Mild_Extensions_1 = torch.optim.Adam(\n",
    "    model_Mild_Extensions_1.parameters(), lr=0.00005, weight_decay=1e-5\n",
    ")\n",
    "\n",
    "criterion_Mild_Extensions_1 = nn.CrossEntropyLoss(\n",
    "    ignore_index=output_vocab[PAD_TOKEN]\n",
    ")\n",
    "\n",
    "# === Train the model using the custom DataLoader and collate function ===\n",
    "# This training loop tracks performance and logs results to file and plot\n",
    "train_model(\n",
    "    model=model_Mild_Extensions_1,\n",
    "    train_loader=train_loader_Mild_Extensions_1,\n",
    "    val_loader=dev_loader_Mild_Extensions_1,\n",
    "    optimizer=optimizer_Mild_Extensions_1,\n",
    "    criterion=criterion_Mild_Extensions_1,\n",
    "    output_vocab=output_vocab_Mild_Extensions_1,\n",
    "    device=device,\n",
    "    num_iters=num_iters,\n",
    "    print_every=print_every,\n",
    "    plot_every=plot_every,\n",
    "    teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
    "    log_filename=\"Sequence-to-Sequence model without attention_Mild_Extensions_1\",\n",
    "    plot_title=\"Sequence-to-Sequence model without attention_Mild_Extensions_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9f79a",
   "metadata": {},
   "source": [
    "#### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6ed54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Baseline predictions saved to _Mild_Extensions_1_output.csv\n"
     ]
    }
   ],
   "source": [
    "model_Mild_Extensions_1.eval()\n",
    "baseline_Mild_Extensions_1_preds= []\n",
    "\n",
    "for i in range(len(test_df_Mild_Extensions_1)):\n",
    "    ingredients__Mild_Extensions_1 = test_df_Mild_Extensions_1.iloc[i]['Ingredients']\n",
    "\n",
    "    # 🔁 Use same preprocessing as training\n",
    "    tokens_Mild_Extensions_1 = preprocess_text(ingredients__Mild_Extensions_1)\n",
    "    input_ids_Mild_Extensions_1 = [input_vocab_Mild_Extensions_1.get(tok, input_vocab_Mild_Extensions_1[UNK_TOKEN]) for tok in tokens_Mild_Extensions_1]\n",
    "    input_ids_Mild_Extensions_1 = input_ids_Mild_Extensions_1[:MAX_INGREDIENT_LEN] + [input_vocab_Mild_Extensions_1[PAD_TOKEN]] * max(0, MAX_INGREDIENT_LEN - len(input_ids_Mild_Extensions_1))\n",
    "\n",
    "    src_tensor_Mild_Extensions_1 = torch.tensor(input_ids_Mild_Extensions_1, dtype=torch.long, device=device)\n",
    "\n",
    "    # 🧠 Generate recipe\n",
    "    generated_recipe_Mild_Extensions_1 = generate_recipe(model_Mild_Extensions_1, src_tensor_Mild_Extensions_1, output_vocab_Mild_Extensions_1, device, max_len = MAX_RECIPE_LEN, use_attention=False)\n",
    "    baseline_Mild_Extensions_1_preds.append(generated_recipe_Mild_Extensions_1)\n",
    "\n",
    "# 🗂️ Add predictions to test DataFrame\n",
    "test_df_Mild_Extensions_1['Recipe - Mild_Extensions_1'] = baseline_Mild_Extensions_1_preds\n",
    "\n",
    "# ✅ Choose columns\n",
    "columns_to_save_Mild_Extensions_1 = ['Ingredients', 'Recipe - Mild_Extensions_1']\n",
    "if 'Recipe' in test_df.columns:\n",
    "    columns_to_save_Mild_Extensions_1.insert(1, 'Recipe')\n",
    "\n",
    "# 💾 Save to timestamped CSV\n",
    "filename_Mild_Extensions_1 = \"_Mild_Extensions_1_output.csv\"\n",
    "test_df_Mild_Extensions_1[columns_to_save_Mild_Extensions_1].to_csv(filename_Mild_Extensions_1, index=False)\n",
    "print(f\"✅ Baseline predictions saved to {filename_Mild_Extensions_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458f565",
   "metadata": {},
   "source": [
    "#### Evalating the result Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e8bba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1081/1081 [00:48<00:00, 22.26it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Appended results to Assignment_2_evaluation_results.csv\n",
      "                           Model      BLEU    METEOR  BERTScore\n",
      "2  Seq2Seq-RNN_Mild_Extensions_1  0.016269  0.098067   0.822164\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test_loader (or dev_loader if preferred)\n",
    "metrics_Mild_Extensions_1 = evaluate_model(model_Mild_Extensions_1, test_loader_Mild_Extensions_1, input_vocab_Mild_Extensions_1, output_vocab_Mild_Extensions_1, device, model_name=\"Seq2Seq-RNN_Mild_Extensions_1\")\n",
    "\n",
    "# Step 2: Load existing results if the file exists\n",
    "results_file = \"Assignment_2_evaluation_results.csv\"\n",
    "if os.path.exists(results_file):\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([metrics_Mild_Extensions_1])], ignore_index=True)\n",
    "else:\n",
    "    results_df = pd.DataFrame([metrics_Mild_Extensions_1])\n",
    "\n",
    "# Step 3: Save back to the same file\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"✅ Appended results to {results_file}\")\n",
    "print(results_df.tail(1))  # show just the latest entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e2011",
   "metadata": {},
   "source": [
    "### Toy Input texts (gold and predicted recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1e8ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_recipe_from_input(model, input_text, input_vocab, output_vocab, device,\n",
    "                              preprocess_fn=preprocess_text,\n",
    "                              max_input_len=MAX_INGREDIENT_LEN,\n",
    "                              max_output_len=MAX_RECIPE_LEN,\n",
    "                              use_attention=False):\n",
    "    \"\"\"\n",
    "    Generate a recipe from an input ingredient list using the provided model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Seq2Seq model.\n",
    "        input_text: Raw ingredient string.\n",
    "        input_vocab: Dictionary mapping input tokens to indices.\n",
    "        output_vocab: Dictionary mapping output indices to tokens.\n",
    "        device: Device to run the model on (CPU or GPU).\n",
    "        preprocess_fn: Preprocessing function used during training.\n",
    "        max_input_len: Maximum length of input sequence.\n",
    "        max_output_len: Maximum length of output recipe.\n",
    "        use_attention: Whether the model uses attention.\n",
    "\n",
    "    Returns:\n",
    "        Generated recipe string.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = preprocess_fn(input_text)\n",
    "    input_ids = [input_vocab.get(tok, input_vocab[UNK_TOKEN]) for tok in tokens]\n",
    "    input_ids = input_ids[:max_input_len] + [input_vocab[PAD_TOKEN]] * max(0, max_input_len - len(input_ids))\n",
    "\n",
    "    src_tensor = torch.tensor(input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_recipe = generate_recipe(model, src_tensor, output_vocab, device,\n",
    "                                           max_len=max_output_len,\n",
    "                                           use_attention=use_attention)\n",
    "    return generated_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9b9f816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Generated Recipe baseline1:\n",
      " mix ingredient together add cup water add cup water add sugar mix well add milk mixture add vanilla mixture pour mixture chill firm\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "user_input = \"sugar, lemon juice,  water,  orange juice, strawberries, icecream\"\n",
    "\n",
    "# Change `model_baseline1` to your actual model\n",
    "output_recipe_baseline1 = predict_recipe_from_input(\n",
    "    model=model_Mild_Extensions_1, \n",
    "    input_text=user_input,\n",
    "    input_vocab=input_vocab_Mild_Extensions_1,\n",
    "    output_vocab=output_vocab_Mild_Extensions_1,\n",
    "    device=device,\n",
    "    use_attention=False  # or True if the model uses attention\n",
    ")\n",
    "print(\"🧾 Generated Recipe baseline1:\\n\", output_recipe_baseline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7c9299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Generated Recipe baseline1:\n",
      " mix together sugar milk add sugar vanilla beat egg white add vanilla vanilla beat egg white add vanilla vanilla beat egg white add vanilla vanilla beat egg white add vanilla vanilla nut pour pie shell chill firm\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "user_input = \"8 oz philadelphia cream cheese, 14 oz can sweetened condensed milk, 1 ts vanilla, 1/3 c  lemon juice, 48 oz canned cherries, 8 inch graham cracker,  pie crusts\"\n",
    "\n",
    "# Change `model_baseline1` to your actual model\n",
    "output_recipe_baseline1 = predict_recipe_from_input(\n",
    "    model=model_Mild_Extensions_1, \n",
    "    input_text=user_input,\n",
    "    input_vocab=input_vocab_Mild_Extensions_1,\n",
    "    output_vocab=output_vocab_Mild_Extensions_1,\n",
    "    device=device,\n",
    "    use_attention=False  # or True if the model uses attention\n",
    ")\n",
    "print(\"🧾 Generated Recipe baseline1:\\n\", output_recipe_baseline1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
