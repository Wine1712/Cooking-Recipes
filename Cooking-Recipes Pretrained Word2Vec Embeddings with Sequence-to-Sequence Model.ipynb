{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f012667",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a66edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & Utilities ---\n",
    "import os                    # File and directory operations\n",
    "import re                    # Regular expressions for string cleaning\n",
    "import string                # String operations\n",
    "import ast                   # Safe evaluation of Python expressions\n",
    "import random                # Random operations (shuffling, sampling)\n",
    "import time                  # Time tracking for performance\n",
    "from collections import Counter  # Count frequencies of tokens\n",
    "\n",
    "# --- Data Handling ---\n",
    "import pandas as pd          # DataFrames for structured data\n",
    "import numpy as np           # Numerical operations and arrays\n",
    "\n",
    "# --- Natural Language Processing (NLP) ---\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize           # Tokenize sentences/words\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.corpus import stopwords, wordnet        # Common stopwords and WordNet\n",
    "from nltk.stem import WordNetLemmatizer           # Lemmatization for word normalization\n",
    "from bert_score import score as bert_score        # Semantic similarity metric\n",
    "\n",
    "# --- Deep Learning with PyTorch ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- Visualization & Progress Tracking ---\n",
    "import matplotlib.pyplot as plt                   # Plotting graphs\n",
    "from tqdm import tqdm                              # Progress bar for loops\n",
    "\n",
    "# --- Word Embeddings ---\n",
    "from gensim.models import KeyedVectors            # Load pretrained word vectors\n",
    "\n",
    "# --- Memory Management ---\n",
    "import gc                                          # Garbage collection to manage RAM\n",
    "\n",
    "# --- Optimization ---\n",
    "import torch.optim as optim                       # Optimizers (e.g., Adam, SGD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168da80c",
   "metadata": {},
   "source": [
    "#### Set up the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df3b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Device Configuration ---\n",
    "# Automatically select the best available device: MPS (Apple), CUDA (GPU), or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(\"✅ Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429480b",
   "metadata": {},
   "source": [
    "#### Special Tokens and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Special Tokens & Model Hyperparameters ---\n",
    "# Define tokens for padding, start/end of sequence, and unknown words\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# Set model hyperparameters and data preprocessing constants\n",
    "MAX_INGREDIENT_LEN = 20     # Max number of input tokens\n",
    "MAX_RECIPE_LEN = 60         # Max number of output tokens\n",
    "BATCH_SIZE = 64             # Training batch size\n",
    "EMB_DIM = 64                # Word embedding dimension\n",
    "HIDDEN_DIM = 256            # RNN hidden state dimension\n",
    "TEACHER_FORCING_RATIO = 0.8 # Probability of using teacher forcing during training\n",
    "num_iters = 10000           # Total training iterations\n",
    "print_every = 50            # Frequency of logging training loss\n",
    "plot_every = 50             # Frequency of computing validation loss and plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868afa1",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ce9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSVs\n",
    "train_df = pd.read_csv('C:/Users/nidhi/Downloads/wine/Cooking_Dataset/train.csv')\n",
    "dev_df = pd.read_csv('C:/Users/nidhi/Downloads/wine/Cooking_Dataset/dev.csv')\n",
    "test_df = pd.read_csv('C:/Users/nidhi/Downloads/wine/Cooking_Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NLP Preprocessing Utilities ---\n",
    "lemmatizer = WordNetLemmatizer()                        # Word lemmatizer for reducing words to base form\n",
    "stop_words = set(stopwords.words('english'))            # English stopwords to remove from text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Cleaning and Formatting Functions ---\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes a text string by lowercasing, removing non-alphabetic characters,\n",
    "    removing stopwords, and applying lemmatization.\n",
    "    \"\"\"\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', str(text).lower())\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return words if words else [\"unknown\"]\n",
    "\n",
    "def preprocess_ingredients_column(df):\n",
    "    \"\"\"Adds 'input_tokens' column by preprocessing the 'Ingredients' field.\"\"\"\n",
    "    df['input_tokens'] = df['Ingredients'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "def preprocess_recipes_column(df):\n",
    "    \"\"\"Adds 'output_tokens' column by preprocessing the 'Recipe' field.\"\"\"\n",
    "    df['output_tokens'] = df['Recipe'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "def format_input_prompt(tokens):\n",
    "    \"\"\"Formats ingredient tokens as a prompt string for generation.\"\"\"\n",
    "    return f\"Generate recipe for: {', '.join(tokens)}\"\n",
    "\n",
    "def format_target_text(row):\n",
    "    \"\"\"Formats a row into a readable multi-line recipe string.\"\"\"\n",
    "    title = row['Title'] if 'Title' in row else 'Generated Recipe'\n",
    "    ingredients = ', '.join(row['input_tokens'])\n",
    "    instructions = ', '.join(row['output_tokens'])\n",
    "    return f\"Title: {title}\\n\\nIngredients:\\n{ingredients}\\nInstructions:\\n{instructions}\"\n",
    "\n",
    "def build_vocab(token_lists):\n",
    "    \"\"\"\n",
    "    Builds vocabulary from token lists and assigns unique index to each token.\n",
    "    Includes special tokens.\n",
    "    \"\"\"\n",
    "    vocab = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
    "    counter = Counter(token for tokens in token_lists for token in tokens)\n",
    "    for token in counter:\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff993b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_ingredients_column(train_df)\n",
    "train_df = preprocess_recipes_column(train_df)\n",
    "\n",
    "dev_df = preprocess_ingredients_column(dev_df)\n",
    "dev_df = preprocess_recipes_column(dev_df)\n",
    "\n",
    "test_df = preprocess_ingredients_column(test_df)\n",
    "test_df = preprocess_recipes_column(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6748177",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['input_prompt'] = train_df['input_tokens'].apply(format_input_prompt)\n",
    "dev_df['input_prompt'] = dev_df['input_tokens'].apply(format_input_prompt)\n",
    "test_df['input_prompt'] = test_df['input_tokens'].apply(format_input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6232ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target_text'] = train_df.apply(format_target_text, axis=1)\n",
    "dev_df['target_text'] = dev_df.apply(format_target_text, axis=1)\n",
    "test_df['target_text'] = test_df.apply(format_target_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1074372",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = build_vocab(train_df['input_tokens'])\n",
    "output_vocab = build_vocab(train_df['output_tokens'])\n",
    "\n",
    "input_idx2word = {i: w for w, i in input_vocab.items()}\n",
    "output_idx2word = {i: w for w, i in output_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c314658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tokens, vocab, max_len, is_target=False):\n",
    "    \"\"\"\n",
    "    Encodes a list of tokens into a fixed-length list of token IDs using a vocabulary.\n",
    "\n",
    "    Args:\n",
    "        tokens: List of tokens (strings) to encode.\n",
    "        vocab: Dictionary mapping tokens to indices.\n",
    "        max_len: Maximum allowed length of the output sequence.\n",
    "        is_target: If True, adds <sos> at the start and <eos> at the end.\n",
    "\n",
    "    Returns:\n",
    "        A list of token IDs (integers), padded or truncated to max_len.\n",
    "    \"\"\"\n",
    "    encoded = []  # Initialize the list of token IDs\n",
    "\n",
    "    if is_target:\n",
    "        # Add <sos> token at the beginning if it's a target sequence\n",
    "        encoded.append(vocab[SOS_TOKEN])\n",
    "    \n",
    "    for tok in tokens:\n",
    "        # Convert each token to its corresponding index in the vocab\n",
    "        # Use <unk> token index if token is not found\n",
    "        encoded.append(vocab.get(tok, vocab[UNK_TOKEN]))\n",
    "\n",
    "    if is_target:\n",
    "        # Add <eos> token at the end if it's a target sequence\n",
    "        encoded.append(vocab[EOS_TOKEN])\n",
    "\n",
    "    # Ensure the sequence is exactly max_len in length\n",
    "    if len(encoded) > max_len:\n",
    "        # Truncate if too long\n",
    "        encoded = encoded[:max_len]\n",
    "    else:\n",
    "        # Pad with <pad> tokens if too short\n",
    "        encoded += [vocab[PAD_TOKEN]] * (max_len - len(encoded))\n",
    "\n",
    "    return encoded  # Return the fixed-length list of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8907583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CookingDataset(Dataset):\n",
    "    def __init__(self, df, input_vocab, output_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): The dataset containing tokenized ingredients and recipes.\n",
    "            input_vocab (dict): Vocabulary mapping for input tokens (ingredients).\n",
    "            output_vocab (dict): Vocabulary mapping for output tokens (recipes).\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples (rows) in the DataFrame\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get token lists from the DataFrame at the given index\n",
    "        src_tokens = self.df.iloc[idx]['input_tokens']   # Ingredients\n",
    "        trg_tokens = self.df.iloc[idx]['output_tokens']  # Recipe steps\n",
    "\n",
    "        # Encode the tokens using the vocabularies\n",
    "        src_encoded = encode_text(src_tokens, self.input_vocab, MAX_INGREDIENT_LEN)\n",
    "        trg_encoded = encode_text(trg_tokens, self.output_vocab, MAX_RECIPE_LEN, is_target=True)\n",
    "\n",
    "        # Return as PyTorch tensors\n",
    "        return torch.tensor(src_encoded), torch.tensor(trg_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d27d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader to pad sequences in a batch.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of (input_tensor, target_tensor) tuples from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        src_padded: Padded input tensor of shape (batch_size, max_input_len).\n",
    "        trg_padded: Padded target tensor of shape (batch_size, max_target_len).\n",
    "    \"\"\"\n",
    "    # Unpack the batch into two lists: inputs and targets\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "\n",
    "    # Pad input sequences to the length of the longest in the batch\n",
    "    # batch_first=True -> shape will be (batch_size, seq_len)\n",
    "    src_padded = pad_sequence(\n",
    "        src_batch, batch_first=True, padding_value=input_vocab[PAD_TOKEN]\n",
    "    )\n",
    "\n",
    "    # Pad target sequences similarly\n",
    "    trg_padded = pad_sequence(\n",
    "        trg_batch, batch_first=True, padding_value=output_vocab[PAD_TOKEN]\n",
    "    )\n",
    "\n",
    "    # Return padded input and target batches\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb03747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CookingDataset(train_df, input_vocab, output_vocab)\n",
    "dev_dataset = CookingDataset(dev_df, input_vocab, output_vocab)\n",
    "test_dataset = CookingDataset(test_df, input_vocab, output_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307b721",
   "metadata": {},
   "source": [
    "#### Epoch Time Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4418ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format time nicely\n",
    "def epoch_time(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Calculates elapsed time between two time points and formats it into minutes and seconds.\n",
    "\n",
    "    Args:\n",
    "        start_time: Float timestamp at the start (e.g., from time.time()).\n",
    "        end_time: Float timestamp at the end.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (minutes, seconds) representing the duration.\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    minutes = int(elapsed_time / 60)\n",
    "    seconds = int(elapsed_time - (minutes * 60))\n",
    "    return minutes, seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b7aa4",
   "metadata": {},
   "source": [
    "#### Evaluating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdb8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line creates a smoothing function for the BLEU score calculation using NLTK’s SmoothingFunction.\n",
    "smoothie = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a443d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, input_vocab, output_vocab, device, model_name=\"Seq2Seq-RNN\"):\n",
    "    \"\"\"\n",
    "    Evaluates a trained Seq2Seq model using BLEU, METEOR, and BERTScore.\n",
    "\n",
    "    Args:\n",
    "        model: The trained sequence-to-sequence model.\n",
    "        data_loader: A DataLoader for the validation or test set.\n",
    "        input_vocab: Vocabulary mapping for input tokens (not used in this function).\n",
    "        output_vocab: Vocabulary mapping for output tokens (used for decoding).\n",
    "        device: PyTorch device (e.g., 'cuda' or 'cpu').\n",
    "        model_name: A string label for the model (used in the results dictionary).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing average BLEU, METEOR, and BERTScore values for the dataset.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store evaluation metrics and text outputs\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Create a reverse mapping from index to word for decoding output tokens\n",
    "    output_idx2word = {i: w for w, i in output_vocab.items()}\n",
    "\n",
    "    # Disable gradient calculation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop through batches from the validation/test DataLoader\n",
    "        for src_batch, trg_batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Move batches to the correct device (CPU or GPU)\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "\n",
    "            # Get model predictions without teacher forcing\n",
    "            output = model(src_batch, trg_batch, teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Get the most likely token index at each time step\n",
    "            # Shape: (batch_size, seq_len)\n",
    "            output_ids = output.argmax(2)\n",
    "\n",
    "            # Loop through each prediction-reference pair in the batch\n",
    "            for pred_seq, true_seq in zip(output_ids, trg_batch):\n",
    "                # Convert predicted token IDs to words, removing special tokens\n",
    "                pred_tokens = [\n",
    "                    output_idx2word.get(idx.item(), UNK_TOKEN)\n",
    "                    for idx in pred_seq\n",
    "                    if idx.item() not in [output_vocab[PAD_TOKEN], output_vocab[SOS_TOKEN], output_vocab[EOS_TOKEN]]\n",
    "                ]\n",
    "\n",
    "                true_tokens = [\n",
    "                    output_idx2word.get(idx.item(), UNK_TOKEN)\n",
    "                    for idx in true_seq\n",
    "                    if idx.item() not in [output_vocab[PAD_TOKEN], output_vocab[SOS_TOKEN], output_vocab[EOS_TOKEN]]\n",
    "                ]\n",
    "\n",
    "                # Store string versions for corpus-level evaluation later\n",
    "                predictions.append(\" \".join(pred_tokens))\n",
    "                references.append(\" \".join(true_tokens))\n",
    "\n",
    "                # Compute individual BLEU and METEOR scores for this sample\n",
    "                bleu = sentence_bleu([true_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "                meteor = meteor_score([true_tokens], pred_tokens)\n",
    "\n",
    "                bleu_scores.append(bleu)\n",
    "                meteor_scores.append(meteor)\n",
    "\n",
    "    # Compute BERTScore once for the entire corpus\n",
    "    try:\n",
    "        P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n",
    "        bert_f1 = F1.mean().item()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ BERTScore skipped due to error: {e}\")\n",
    "        bert_f1 = None\n",
    "\n",
    "    # Compute average scores across all samples\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"METEOR\": sum(meteor_scores) / len(meteor_scores),\n",
    "        \"BERTScore\": bert_f1 if bert_f1 is not None else \"Unavailable\"\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8f820",
   "metadata": {},
   "source": [
    "#### Evaluation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef0bb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, val_loader, criterion, output_vocab, device):\n",
    "    \"\"\"\n",
    "    Computes the average loss of a trained model on a validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Seq2Seq model.\n",
    "        val_loader: DataLoader for the validation set.\n",
    "        criterion: Loss function (e.g., nn.CrossEntropyLoss).\n",
    "        output_vocab: Target vocabulary dictionary (not used directly here).\n",
    "        device: PyTorch device (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        Average validation loss across all batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src_batch, trg_batch in val_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "\n",
    "            output = model(src_batch, trg_batch, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            target = trg_batch[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef344f7",
   "metadata": {},
   "source": [
    "#### Training Loop for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, output_vocab, device,\n",
    "                num_iters, print_every, plot_every, teacher_forcing_ratio=0.5, \n",
    "                log_filename=\"training_log.csv\", plot_title=\"Training and Validation Loss\"):\n",
    "    \"\"\"\n",
    "    Trains a Seq2Seq model using random batches from the training data.\n",
    "    Also tracks validation loss and logs progress over time.\n",
    "    \"\"\"\n",
    "    \n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    print_loss_total = 0  # Running total for printing\n",
    "    plot_loss_total = 0   # Running total for plotting\n",
    "    first_val_loss = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Cache all training batches to randomly sample from them\n",
    "    train_batches = list(train_loader)\n",
    "\n",
    "    for iteration in range(1, num_iters + 1):\n",
    "        # === Randomly sample a batch from training set ===\n",
    "        input_batch, target_batch = random.choice(train_batches)\n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # === Forward pass ===\n",
    "        output = model(input_batch, target_batch, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "        # === Reshape output and target for loss computation ===\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)      # Skip <sos>\n",
    "        target = target_batch[:, 1:].reshape(-1)\n",
    "\n",
    "        # === Compute loss and backpropagation ===\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        print_loss_total += loss_value\n",
    "        plot_loss_total += loss_value\n",
    "\n",
    "        # === Print training progress every 'print_every' iterations ===\n",
    "        if iteration % print_every == 0:\n",
    "            avg_loss = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"[Iter {iteration:05d}] ⏱ Time: {elapsed:.1f}s | Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # === Validate and track performance every 'plot_every' iterations ===\n",
    "        if iteration % plot_every == 0:\n",
    "            val_loss_total = 0\n",
    "            model.eval()  # Set model to evaluation mode (e.g., disable dropout)\n",
    "            with torch.no_grad():  # Disable gradient tracking for validation\n",
    "                for val_input, val_target in val_loader:\n",
    "                    val_input = val_input.to(device)\n",
    "                    val_target = val_target.to(device)\n",
    "\n",
    "                    # In validation, use greedy decoding (no teacher forcing)\n",
    "                    val_output = model(val_input, val_target, teacher_forcing_ratio=0.0)\n",
    "                    val_output = val_output[:, 1:].reshape(-1, output_dim)\n",
    "                    val_target = val_target[:, 1:].reshape(-1)\n",
    "                    val_loss = criterion(val_output, val_target)\n",
    "                    val_loss_total += val_loss.item()\n",
    "\n",
    "            # Compute and store average validation loss\n",
    "            avg_val_loss = val_loss_total / len(val_loader)\n",
    "            train_losses.append(plot_loss_total / plot_every)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print(f\"📉 Iter {iteration}: Val Loss = {avg_val_loss:.4f}\")\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    # === Save training and validation loss log to CSV ===\n",
    "    log_data = {\n",
    "        \"Iteration\": list(range(plot_every, plot_every * len(train_losses) + 1, plot_every)),\n",
    "        \"Training Loss\": train_losses,\n",
    "        \"Validation Loss\": val_losses\n",
    "    }\n",
    "    log_df = pd.DataFrame(log_data)\n",
    "    log_df.to_csv(log_filename, index=False)\n",
    "    print(f\"📁 Training log saved to {log_filename}\")\n",
    "\n",
    "    # === Plot training vs. validation loss ===\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(log_data[\"Iteration\"], train_losses, label='Training Loss')\n",
    "    plt.plot(log_data[\"Iteration\"], val_losses, label='Validation Loss')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae420b",
   "metadata": {},
   "source": [
    "#### Generating the Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c34cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, src_tensor, trg_vocab, device, max_len=MAX_RECIPE_LEN, use_attention=False):\n",
    "    \"\"\"\n",
    "    Generates a recipe from a single input using either attention-based or vanilla seq2seq.\n",
    "\n",
    "    Args:\n",
    "        model: full seq2seq model or tuple (encoder, decoder)\n",
    "        src_tensor: tensor of shape (src_len,)\n",
    "        trg_vocab: target vocabulary (word to index)\n",
    "        device: torch device (cpu/cuda)\n",
    "        max_len: max length of the generated recipe\n",
    "        use_attention: set to True if using (encoder, decoder) attention-style models\n",
    "\n",
    "    Returns:\n",
    "        String of generated recipe text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    trg_vocab_inv = {i: w for w, i in trg_vocab.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension: (1, src_len)\n",
    "        src_tensor = src_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        if use_attention:\n",
    "            # Expecting (encoder, decoder) as a tuple\n",
    "            encoder, decoder = model\n",
    "            encoder_outputs, hidden = encoder(src_tensor)\n",
    "\n",
    "            input_token = torch.tensor([trg_vocab[SOS_TOKEN]], device=device)\n",
    "            result = []\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                output, hidden, _ = decoder(input_token, hidden, encoder_outputs)\n",
    "                top1 = output.argmax(1).item()\n",
    "\n",
    "                if top1 == trg_vocab[EOS_TOKEN]:\n",
    "                    break\n",
    "                result.append(trg_vocab_inv.get(top1, UNK_TOKEN))\n",
    "                input_token = torch.tensor([top1], device=device)\n",
    "\n",
    "        else:\n",
    "            # Expecting model to be Seq2Seq class with internal encoder+decoder\n",
    "            dummy_trg = torch.zeros((1, max_len), dtype=torch.long, device=device)\n",
    "            output = model(src_tensor, dummy_trg, teacher_forcing_ratio=0.0)\n",
    "            pred_ids = output.argmax(2).squeeze(0).tolist()\n",
    "\n",
    "            result = []\n",
    "            for idx in pred_ids:\n",
    "                token = trg_vocab_inv.get(idx, UNK_TOKEN)\n",
    "                if token == EOS_TOKEN:\n",
    "                    break\n",
    "                if token not in [PAD_TOKEN, SOS_TOKEN]:\n",
    "                    result.append(token)\n",
    "\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb619c",
   "metadata": {},
   "source": [
    "## The 2 Mild Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff2313",
   "metadata": {},
   "source": [
    "### Using Pretrained Word2Vec Embeddings with Baseline 1 — Sequence-to-Sequence Model without Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da2f19",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a24ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNNLSTMWithPretrained(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder that combines RNN and LSTM, using a pretrained embedding matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_matrix, hidden_dim):\n",
    "        super(EncoderRNNLSTMWithPretrained, self).__init__()\n",
    "\n",
    "        emb_matrix = torch.FloatTensor(emb_matrix)\n",
    "        vocab_size, emb_dim = emb_matrix.shape\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze=False)\n",
    "        \n",
    "        # RNN followed by LSTM\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden_size = hidden_dim\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (batch_size, seq_len)\n",
    "        embedded = self.embedding(src)                 # (batch_size, seq_len, emb_dim)\n",
    "\n",
    "        rnn_out, rnn_hidden = self.rnn(embedded)       # (batch_size, seq_len, hidden_dim)\n",
    "        lstm_out, (hidden, cell) = self.lstm(rnn_out)  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        return lstm_out, (hidden, cell)\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b0e4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder with both RNN and LSTM layers.\n",
    "    Input goes through RNN, then LSTM, then linear output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim):\n",
    "        super(DecoderRNNLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.hidden_size = hidden_dim  # for initHidden()\n",
    "\n",
    "    def forward(self, input_step, hidden, cell):\n",
    "        # input_step: (batch_size, 1)\n",
    "        embedded = self.dropout(self.embedding(input_step))  # (batch_size, 1, emb_dim)\n",
    "\n",
    "        # Pass through RNN first\n",
    "        rnn_output, _ = self.rnn(embedded)  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # Then through LSTM\n",
    "        lstm_output, (hidden, cell) = self.lstm(rnn_output, (hidden, cell))  # (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # Final prediction\n",
    "        prediction = self.fc_out(lstm_output.squeeze(1))  # (batch_size, output_dim)\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "    def initHidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca617f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input_step, hidden, cell):\n",
    "    embedded = self.dropout(self.embedding(input_step))\n",
    "    rnn_output, _ = self.rnn(embedded)\n",
    "    lstm_output, (hidden, cell) = self.lstm(rnn_output, (hidden, cell))\n",
    "    prediction = self.fc_out(lstm_output.squeeze(1))\n",
    "    return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "745dd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Mild_Extensions(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq_Mild_Extensions, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: (batch_size, src_len)\n",
    "        # trg: (batch_size, trg_len)\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        # Prepare output tensor\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Encode\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "\n",
    "        # First decoder input = <sos>\n",
    "        input_token = trg[:, 0].unsqueeze(1)  # shape: (batch_size, 1)\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            # Decide next input\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "\n",
    "            input_token = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94ee53",
   "metadata": {},
   "source": [
    "#### Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eafdff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained embeddings (GoogleNews-vectors-negative300.bin must be downloaded beforehand)\n",
    "word2vec_path = \"C:/Users/nidhi/Downloads/wine/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "EMBEDDING_DIM_word2vec = 300  # Must match pretrained dim\n",
    "vocab_size = len(input_vocab)\n",
    "embedding_matrix_word2vec = np.zeros((vocab_size, EMBEDDING_DIM_word2vec))\n",
    "for word, idx in input_vocab.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix_word2vec[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix_word2vec[idx] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM_word2vec,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989e6d2",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 00050] ⏱ Time: 26.0s | Avg Train Loss: 9.3289\n",
      "📉 Iter 50: Val Loss = 9.2697\n",
      "[Iter 00100] ⏱ Time: 67.7s | Avg Train Loss: 8.9477\n",
      "📉 Iter 100: Val Loss = 7.8866\n",
      "[Iter 00150] ⏱ Time: 108.2s | Avg Train Loss: 7.2708\n",
      "📉 Iter 150: Val Loss = 6.5141\n",
      "[Iter 00200] ⏱ Time: 148.6s | Avg Train Loss: 6.4214\n",
      "📉 Iter 200: Val Loss = 6.1562\n",
      "[Iter 00250] ⏱ Time: 189.4s | Avg Train Loss: 6.1730\n",
      "📉 Iter 250: Val Loss = 6.0423\n",
      "[Iter 00300] ⏱ Time: 235.5s | Avg Train Loss: 6.1089\n",
      "📉 Iter 300: Val Loss = 5.9927\n",
      "[Iter 00350] ⏱ Time: 463.6s | Avg Train Loss: 6.0700\n",
      "📉 Iter 350: Val Loss = 5.9669\n",
      "[Iter 00400] ⏱ Time: 743.5s | Avg Train Loss: 6.0427\n",
      "📉 Iter 400: Val Loss = 5.9553\n",
      "[Iter 00450] ⏱ Time: 982.1s | Avg Train Loss: 6.0577\n",
      "📉 Iter 450: Val Loss = 5.9458\n",
      "[Iter 00500] ⏱ Time: 1204.4s | Avg Train Loss: 6.0276\n",
      "📉 Iter 500: Val Loss = 5.9383\n",
      "[Iter 00550] ⏱ Time: 1427.9s | Avg Train Loss: 6.0436\n",
      "📉 Iter 550: Val Loss = 5.9313\n",
      "[Iter 00600] ⏱ Time: 1675.1s | Avg Train Loss: 6.0221\n",
      "📉 Iter 600: Val Loss = 5.9252\n",
      "[Iter 00650] ⏱ Time: 1911.2s | Avg Train Loss: 5.9963\n",
      "📉 Iter 650: Val Loss = 5.9170\n",
      "[Iter 00700] ⏱ Time: 2117.1s | Avg Train Loss: 6.0118\n",
      "📉 Iter 700: Val Loss = 5.9122\n",
      "[Iter 00750] ⏱ Time: 2241.0s | Avg Train Loss: 5.9904\n",
      "📉 Iter 750: Val Loss = 5.9094\n",
      "[Iter 00800] ⏱ Time: 2314.5s | Avg Train Loss: 5.9876\n",
      "📉 Iter 800: Val Loss = 5.9038\n",
      "[Iter 00850] ⏱ Time: 2390.1s | Avg Train Loss: 5.9920\n",
      "📉 Iter 850: Val Loss = 5.9071\n",
      "[Iter 00900] ⏱ Time: 2464.8s | Avg Train Loss: 5.9661\n",
      "📉 Iter 900: Val Loss = 5.9201\n",
      "[Iter 00950] ⏱ Time: 2537.9s | Avg Train Loss: 5.9427\n",
      "📉 Iter 950: Val Loss = 5.9172\n",
      "[Iter 01000] ⏱ Time: 2610.3s | Avg Train Loss: 5.9420\n",
      "📉 Iter 1000: Val Loss = 5.9147\n",
      "[Iter 01050] ⏱ Time: 2681.6s | Avg Train Loss: 5.9558\n",
      "📉 Iter 1050: Val Loss = 5.9163\n",
      "[Iter 01100] ⏱ Time: 2753.4s | Avg Train Loss: 5.9192\n",
      "📉 Iter 1100: Val Loss = 5.9158\n",
      "[Iter 01150] ⏱ Time: 2828.9s | Avg Train Loss: 5.9006\n",
      "📉 Iter 1150: Val Loss = 5.9171\n",
      "[Iter 01200] ⏱ Time: 2903.2s | Avg Train Loss: 5.9079\n",
      "📉 Iter 1200: Val Loss = 5.9155\n",
      "[Iter 01250] ⏱ Time: 2977.4s | Avg Train Loss: 5.9229\n",
      "📉 Iter 1250: Val Loss = 5.9119\n",
      "[Iter 01300] ⏱ Time: 3056.4s | Avg Train Loss: 5.8914\n",
      "📉 Iter 1300: Val Loss = 5.9153\n",
      "[Iter 01350] ⏱ Time: 3129.4s | Avg Train Loss: 5.8780\n",
      "📉 Iter 1350: Val Loss = 5.9058\n",
      "[Iter 01400] ⏱ Time: 3201.7s | Avg Train Loss: 5.8794\n",
      "📉 Iter 1400: Val Loss = 5.9162\n",
      "[Iter 01450] ⏱ Time: 3278.7s | Avg Train Loss: 5.8712\n",
      "📉 Iter 1450: Val Loss = 5.9171\n",
      "[Iter 01500] ⏱ Time: 3352.9s | Avg Train Loss: 5.8517\n",
      "📉 Iter 1500: Val Loss = 5.9034\n",
      "[Iter 01550] ⏱ Time: 3427.5s | Avg Train Loss: 5.8657\n",
      "📉 Iter 1550: Val Loss = 5.9071\n",
      "[Iter 01600] ⏱ Time: 3504.9s | Avg Train Loss: 5.8460\n",
      "📉 Iter 1600: Val Loss = 5.9036\n",
      "[Iter 01650] ⏱ Time: 3583.2s | Avg Train Loss: 5.8304\n",
      "📉 Iter 1650: Val Loss = 5.9077\n",
      "[Iter 01700] ⏱ Time: 3658.7s | Avg Train Loss: 5.8134\n",
      "📉 Iter 1700: Val Loss = 5.9065\n",
      "[Iter 01750] ⏱ Time: 3736.3s | Avg Train Loss: 5.8185\n",
      "📉 Iter 1750: Val Loss = 5.9009\n",
      "[Iter 01800] ⏱ Time: 3809.8s | Avg Train Loss: 5.7966\n",
      "📉 Iter 1800: Val Loss = 5.9019\n",
      "[Iter 01850] ⏱ Time: 3882.5s | Avg Train Loss: 5.7861\n",
      "📉 Iter 1850: Val Loss = 5.9007\n",
      "[Iter 01900] ⏱ Time: 3958.3s | Avg Train Loss: 5.7826\n",
      "📉 Iter 1900: Val Loss = 5.8996\n",
      "[Iter 01950] ⏱ Time: 4035.7s | Avg Train Loss: 5.7584\n",
      "📉 Iter 1950: Val Loss = 5.8996\n",
      "[Iter 02000] ⏱ Time: 4110.5s | Avg Train Loss: 5.7585\n",
      "📉 Iter 2000: Val Loss = 5.9019\n",
      "[Iter 02050] ⏱ Time: 4184.1s | Avg Train Loss: 5.7656\n",
      "📉 Iter 2050: Val Loss = 5.8978\n",
      "[Iter 02100] ⏱ Time: 4255.7s | Avg Train Loss: 5.7433\n",
      "📉 Iter 2100: Val Loss = 5.8923\n",
      "[Iter 02150] ⏱ Time: 4327.9s | Avg Train Loss: 5.7296\n",
      "📉 Iter 2150: Val Loss = 5.9010\n",
      "[Iter 02200] ⏱ Time: 4399.7s | Avg Train Loss: 5.7120\n",
      "📉 Iter 2200: Val Loss = 5.8965\n",
      "[Iter 02250] ⏱ Time: 4471.9s | Avg Train Loss: 5.7046\n",
      "📉 Iter 2250: Val Loss = 5.8977\n",
      "[Iter 02300] ⏱ Time: 4544.1s | Avg Train Loss: 5.6710\n",
      "📉 Iter 2300: Val Loss = 5.8906\n",
      "[Iter 02350] ⏱ Time: 4615.3s | Avg Train Loss: 5.6714\n",
      "📉 Iter 2350: Val Loss = 5.8908\n",
      "[Iter 02400] ⏱ Time: 4686.7s | Avg Train Loss: 5.6600\n",
      "📉 Iter 2400: Val Loss = 5.8882\n",
      "[Iter 02450] ⏱ Time: 4759.5s | Avg Train Loss: 5.6582\n",
      "📉 Iter 2450: Val Loss = 5.8861\n",
      "[Iter 02500] ⏱ Time: 4832.9s | Avg Train Loss: 5.6496\n",
      "📉 Iter 2500: Val Loss = 5.8734\n",
      "[Iter 02550] ⏱ Time: 4905.7s | Avg Train Loss: 5.6304\n",
      "📉 Iter 2550: Val Loss = 5.8558\n",
      "[Iter 02600] ⏱ Time: 4979.4s | Avg Train Loss: 5.6007\n",
      "📉 Iter 2600: Val Loss = 5.8589\n",
      "[Iter 02650] ⏱ Time: 5051.2s | Avg Train Loss: 5.5913\n",
      "📉 Iter 2650: Val Loss = 5.8373\n",
      "[Iter 02700] ⏱ Time: 5169.6s | Avg Train Loss: 5.5915\n",
      "📉 Iter 2700: Val Loss = 5.8441\n",
      "[Iter 02750] ⏱ Time: 5379.5s | Avg Train Loss: 5.5752\n",
      "📉 Iter 2750: Val Loss = 5.8554\n",
      "[Iter 02800] ⏱ Time: 5554.6s | Avg Train Loss: 5.5587\n",
      "📉 Iter 2800: Val Loss = 5.8276\n",
      "[Iter 02850] ⏱ Time: 5735.6s | Avg Train Loss: 5.5417\n",
      "📉 Iter 2850: Val Loss = 5.8242\n",
      "[Iter 02900] ⏱ Time: 5931.8s | Avg Train Loss: 5.5307\n",
      "📉 Iter 2900: Val Loss = 5.8536\n",
      "[Iter 02950] ⏱ Time: 6124.1s | Avg Train Loss: 5.5573\n",
      "📉 Iter 2950: Val Loss = 5.8410\n",
      "[Iter 03000] ⏱ Time: 6306.6s | Avg Train Loss: 5.5185\n",
      "📉 Iter 3000: Val Loss = 5.8310\n",
      "[Iter 03050] ⏱ Time: 6499.7s | Avg Train Loss: 5.4986\n",
      "📉 Iter 3050: Val Loss = 5.8322\n",
      "[Iter 03100] ⏱ Time: 7426.3s | Avg Train Loss: 5.5132\n",
      "📉 Iter 3100: Val Loss = 5.8505\n",
      "[Iter 03150] ⏱ Time: 7470.3s | Avg Train Loss: 5.4977\n",
      "📉 Iter 3150: Val Loss = 5.8171\n",
      "[Iter 03200] ⏱ Time: 7509.2s | Avg Train Loss: 5.4655\n",
      "📉 Iter 3200: Val Loss = 5.8207\n",
      "[Iter 03250] ⏱ Time: 7547.3s | Avg Train Loss: 5.4564\n",
      "📉 Iter 3250: Val Loss = 5.8022\n",
      "[Iter 03300] ⏱ Time: 7587.4s | Avg Train Loss: 5.4533\n",
      "📉 Iter 3300: Val Loss = 5.8444\n",
      "[Iter 03350] ⏱ Time: 7625.9s | Avg Train Loss: 5.4559\n",
      "📉 Iter 3350: Val Loss = 5.8149\n",
      "[Iter 03400] ⏱ Time: 7664.4s | Avg Train Loss: 5.4624\n",
      "📉 Iter 3400: Val Loss = 5.8009\n",
      "[Iter 03450] ⏱ Time: 7703.2s | Avg Train Loss: 5.4705\n",
      "📉 Iter 3450: Val Loss = 5.8277\n",
      "[Iter 03500] ⏱ Time: 7742.0s | Avg Train Loss: 5.4318\n",
      "📉 Iter 3500: Val Loss = 5.8128\n",
      "[Iter 03550] ⏱ Time: 7781.6s | Avg Train Loss: 5.4058\n",
      "📉 Iter 3550: Val Loss = 5.7800\n",
      "[Iter 03600] ⏱ Time: 7820.8s | Avg Train Loss: 5.3873\n",
      "📉 Iter 3600: Val Loss = 5.7784\n",
      "[Iter 03650] ⏱ Time: 7860.0s | Avg Train Loss: 5.3797\n",
      "📉 Iter 3650: Val Loss = 5.7957\n",
      "[Iter 03700] ⏱ Time: 7900.3s | Avg Train Loss: 5.3792\n",
      "📉 Iter 3700: Val Loss = 5.7847\n",
      "[Iter 03750] ⏱ Time: 7939.8s | Avg Train Loss: 5.3912\n",
      "📉 Iter 3750: Val Loss = 5.7913\n",
      "[Iter 03800] ⏱ Time: 7979.3s | Avg Train Loss: 5.3626\n",
      "📉 Iter 3800: Val Loss = 5.7795\n",
      "[Iter 03850] ⏱ Time: 8018.9s | Avg Train Loss: 5.3426\n",
      "📉 Iter 3850: Val Loss = 5.7949\n",
      "[Iter 03900] ⏱ Time: 8058.9s | Avg Train Loss: 5.3717\n",
      "📉 Iter 3900: Val Loss = 5.7825\n",
      "[Iter 03950] ⏱ Time: 8098.5s | Avg Train Loss: 5.3470\n",
      "📉 Iter 3950: Val Loss = 5.7858\n",
      "[Iter 04000] ⏱ Time: 8138.9s | Avg Train Loss: 5.3327\n",
      "📉 Iter 4000: Val Loss = 5.7967\n",
      "[Iter 04050] ⏱ Time: 8178.4s | Avg Train Loss: 5.3181\n",
      "📉 Iter 4050: Val Loss = 5.7847\n",
      "[Iter 04100] ⏱ Time: 8217.7s | Avg Train Loss: 5.3254\n",
      "📉 Iter 4100: Val Loss = 5.7996\n",
      "[Iter 04150] ⏱ Time: 8257.4s | Avg Train Loss: 5.3153\n",
      "📉 Iter 4150: Val Loss = 5.8097\n",
      "[Iter 04200] ⏱ Time: 8297.0s | Avg Train Loss: 5.2994\n",
      "📉 Iter 4200: Val Loss = 5.7919\n",
      "[Iter 04250] ⏱ Time: 8336.8s | Avg Train Loss: 5.3115\n",
      "📉 Iter 4250: Val Loss = 5.7987\n",
      "[Iter 04300] ⏱ Time: 8376.4s | Avg Train Loss: 5.2910\n",
      "📉 Iter 4300: Val Loss = 5.8003\n",
      "[Iter 04350] ⏱ Time: 8416.1s | Avg Train Loss: 5.2740\n",
      "📉 Iter 4350: Val Loss = 5.7996\n",
      "[Iter 04400] ⏱ Time: 8456.0s | Avg Train Loss: 5.2836\n",
      "📉 Iter 4400: Val Loss = 5.8017\n",
      "[Iter 04450] ⏱ Time: 8541.0s | Avg Train Loss: 5.2564\n",
      "📉 Iter 4450: Val Loss = 5.7880\n",
      "[Iter 04500] ⏱ Time: 8652.0s | Avg Train Loss: 5.3013\n",
      "📉 Iter 4500: Val Loss = 5.7994\n",
      "[Iter 04550] ⏱ Time: 8763.1s | Avg Train Loss: 5.2416\n",
      "📉 Iter 4550: Val Loss = 5.8068\n",
      "[Iter 04600] ⏱ Time: 8850.8s | Avg Train Loss: 5.2459\n",
      "📉 Iter 4600: Val Loss = 5.8103\n",
      "[Iter 04650] ⏱ Time: 8890.2s | Avg Train Loss: 5.2232\n",
      "📉 Iter 4650: Val Loss = 5.8074\n",
      "[Iter 04700] ⏱ Time: 8929.4s | Avg Train Loss: 5.2423\n",
      "📉 Iter 4700: Val Loss = 5.7992\n",
      "[Iter 04750] ⏱ Time: 8968.7s | Avg Train Loss: 5.2432\n",
      "📉 Iter 4750: Val Loss = 5.7969\n",
      "[Iter 04800] ⏱ Time: 9008.2s | Avg Train Loss: 5.2068\n",
      "📉 Iter 4800: Val Loss = 5.8057\n",
      "[Iter 04850] ⏱ Time: 9047.7s | Avg Train Loss: 5.1970\n",
      "📉 Iter 4850: Val Loss = 5.8175\n",
      "[Iter 04900] ⏱ Time: 9087.1s | Avg Train Loss: 5.2040\n",
      "📉 Iter 4900: Val Loss = 5.8047\n",
      "[Iter 04950] ⏱ Time: 9127.0s | Avg Train Loss: 5.1927\n",
      "📉 Iter 4950: Val Loss = 5.7999\n",
      "[Iter 05000] ⏱ Time: 9166.6s | Avg Train Loss: 5.1763\n",
      "📉 Iter 5000: Val Loss = 5.8070\n",
      "[Iter 05050] ⏱ Time: 9206.0s | Avg Train Loss: 5.2092\n",
      "📉 Iter 5050: Val Loss = 5.8096\n",
      "[Iter 05100] ⏱ Time: 9246.0s | Avg Train Loss: 5.1864\n",
      "📉 Iter 5100: Val Loss = 5.8036\n",
      "[Iter 05150] ⏱ Time: 9286.0s | Avg Train Loss: 5.1809\n",
      "📉 Iter 5150: Val Loss = 5.7885\n",
      "[Iter 05200] ⏱ Time: 9325.8s | Avg Train Loss: 5.1591\n",
      "📉 Iter 5200: Val Loss = 5.8065\n",
      "[Iter 05250] ⏱ Time: 9365.8s | Avg Train Loss: 5.1749\n",
      "📉 Iter 5250: Val Loss = 5.7991\n",
      "[Iter 05300] ⏱ Time: 9405.6s | Avg Train Loss: 5.1537\n",
      "📉 Iter 5300: Val Loss = 5.7842\n",
      "[Iter 05350] ⏱ Time: 9445.4s | Avg Train Loss: 5.1465\n",
      "📉 Iter 5350: Val Loss = 5.8010\n",
      "[Iter 05400] ⏱ Time: 9485.3s | Avg Train Loss: 5.1562\n",
      "📉 Iter 5400: Val Loss = 5.8149\n",
      "[Iter 05450] ⏱ Time: 9524.8s | Avg Train Loss: 5.1593\n",
      "📉 Iter 5450: Val Loss = 5.8148\n",
      "[Iter 05500] ⏱ Time: 9564.6s | Avg Train Loss: 5.1281\n",
      "📉 Iter 5500: Val Loss = 5.8090\n",
      "[Iter 05550] ⏱ Time: 9604.1s | Avg Train Loss: 5.1237\n",
      "📉 Iter 5550: Val Loss = 5.7973\n",
      "[Iter 05600] ⏱ Time: 9643.6s | Avg Train Loss: 5.1199\n",
      "📉 Iter 5600: Val Loss = 5.8077\n",
      "[Iter 05650] ⏱ Time: 9683.5s | Avg Train Loss: 5.0987\n",
      "📉 Iter 5650: Val Loss = 5.7947\n",
      "[Iter 05700] ⏱ Time: 9723.5s | Avg Train Loss: 5.1293\n",
      "📉 Iter 5700: Val Loss = 5.8035\n",
      "[Iter 05750] ⏱ Time: 9763.8s | Avg Train Loss: 5.1012\n",
      "📉 Iter 5750: Val Loss = 5.7846\n",
      "[Iter 05800] ⏱ Time: 9804.0s | Avg Train Loss: 5.1161\n",
      "📉 Iter 5800: Val Loss = 5.7779\n",
      "[Iter 05850] ⏱ Time: 9851.7s | Avg Train Loss: 5.0923\n",
      "📉 Iter 5850: Val Loss = 5.7896\n",
      "[Iter 05900] ⏱ Time: 9892.1s | Avg Train Loss: 5.0687\n",
      "📉 Iter 5900: Val Loss = 5.7976\n",
      "[Iter 05950] ⏱ Time: 9931.5s | Avg Train Loss: 5.0513\n",
      "📉 Iter 5950: Val Loss = 5.7839\n",
      "[Iter 06000] ⏱ Time: 9970.9s | Avg Train Loss: 5.0727\n",
      "📉 Iter 6000: Val Loss = 5.7943\n",
      "[Iter 06050] ⏱ Time: 10010.2s | Avg Train Loss: 5.0658\n",
      "📉 Iter 6050: Val Loss = 5.7787\n",
      "[Iter 06100] ⏱ Time: 10049.7s | Avg Train Loss: 5.0813\n",
      "📉 Iter 6100: Val Loss = 5.7845\n",
      "[Iter 06150] ⏱ Time: 10089.2s | Avg Train Loss: 5.0708\n",
      "📉 Iter 6150: Val Loss = 5.7905\n",
      "[Iter 06200] ⏱ Time: 10128.7s | Avg Train Loss: 5.0360\n",
      "📉 Iter 6200: Val Loss = 5.7986\n",
      "[Iter 06250] ⏱ Time: 10168.8s | Avg Train Loss: 5.0385\n",
      "📉 Iter 6250: Val Loss = 5.7889\n",
      "[Iter 06300] ⏱ Time: 10208.7s | Avg Train Loss: 5.0430\n",
      "📉 Iter 6300: Val Loss = 5.7881\n",
      "[Iter 06350] ⏱ Time: 10248.6s | Avg Train Loss: 5.0509\n",
      "📉 Iter 6350: Val Loss = 5.7851\n",
      "[Iter 06400] ⏱ Time: 10288.3s | Avg Train Loss: 5.0181\n",
      "📉 Iter 6400: Val Loss = 5.7882\n",
      "[Iter 06450] ⏱ Time: 10328.3s | Avg Train Loss: 5.0101\n",
      "📉 Iter 6450: Val Loss = 5.7832\n",
      "[Iter 06500] ⏱ Time: 10368.5s | Avg Train Loss: 5.0329\n",
      "📉 Iter 6500: Val Loss = 5.7950\n",
      "[Iter 06550] ⏱ Time: 10408.1s | Avg Train Loss: 5.0170\n",
      "📉 Iter 6550: Val Loss = 5.7716\n",
      "[Iter 06600] ⏱ Time: 10448.1s | Avg Train Loss: 5.0048\n",
      "📉 Iter 6600: Val Loss = 5.7665\n",
      "[Iter 06650] ⏱ Time: 10487.3s | Avg Train Loss: 5.0090\n",
      "📉 Iter 6650: Val Loss = 5.7742\n",
      "[Iter 06700] ⏱ Time: 10552.4s | Avg Train Loss: 4.9805\n",
      "📉 Iter 6700: Val Loss = 5.7768\n",
      "[Iter 06750] ⏱ Time: 10658.4s | Avg Train Loss: 5.0138\n",
      "📉 Iter 6750: Val Loss = 5.7665\n",
      "[Iter 06800] ⏱ Time: 10763.4s | Avg Train Loss: 4.9677\n",
      "📉 Iter 6800: Val Loss = 5.7794\n",
      "[Iter 06850] ⏱ Time: 10870.6s | Avg Train Loss: 5.0128\n",
      "📉 Iter 6850: Val Loss = 5.7639\n",
      "[Iter 06900] ⏱ Time: 10978.3s | Avg Train Loss: 5.0107\n",
      "📉 Iter 6900: Val Loss = 5.7680\n",
      "[Iter 06950] ⏱ Time: 11090.5s | Avg Train Loss: 4.9740\n",
      "📉 Iter 6950: Val Loss = 5.7745\n",
      "[Iter 07000] ⏱ Time: 11195.9s | Avg Train Loss: 4.9798\n",
      "📉 Iter 7000: Val Loss = 5.7841\n",
      "[Iter 07050] ⏱ Time: 11300.8s | Avg Train Loss: 4.9700\n",
      "📉 Iter 7050: Val Loss = 5.7669\n",
      "[Iter 07100] ⏱ Time: 11355.2s | Avg Train Loss: 4.9576\n",
      "📉 Iter 7100: Val Loss = 5.7688\n",
      "[Iter 07150] ⏱ Time: 11394.2s | Avg Train Loss: 4.9744\n",
      "📉 Iter 7150: Val Loss = 5.7725\n",
      "[Iter 07200] ⏱ Time: 11433.7s | Avg Train Loss: 4.9361\n",
      "📉 Iter 7200: Val Loss = 5.7476\n",
      "[Iter 07250] ⏱ Time: 11473.0s | Avg Train Loss: 4.9483\n",
      "📉 Iter 7250: Val Loss = 5.7680\n",
      "[Iter 07300] ⏱ Time: 11512.4s | Avg Train Loss: 4.9563\n",
      "📉 Iter 7300: Val Loss = 5.7754\n",
      "[Iter 07350] ⏱ Time: 11551.8s | Avg Train Loss: 4.9273\n",
      "📉 Iter 7350: Val Loss = 5.7707\n",
      "[Iter 07400] ⏱ Time: 11591.8s | Avg Train Loss: 4.9348\n",
      "📉 Iter 7400: Val Loss = 5.7522\n",
      "[Iter 07450] ⏱ Time: 11683.9s | Avg Train Loss: 4.9452\n",
      "📉 Iter 7450: Val Loss = 5.7790\n",
      "[Iter 07500] ⏱ Time: 11723.9s | Avg Train Loss: 4.9340\n",
      "📉 Iter 7500: Val Loss = 5.7558\n",
      "[Iter 07550] ⏱ Time: 11839.8s | Avg Train Loss: 4.8879\n",
      "📉 Iter 7550: Val Loss = 5.7617\n",
      "[Iter 07600] ⏱ Time: 11946.2s | Avg Train Loss: 4.9061\n",
      "📉 Iter 7600: Val Loss = 5.7646\n",
      "[Iter 07650] ⏱ Time: 12051.3s | Avg Train Loss: 4.9059\n",
      "📉 Iter 7650: Val Loss = 5.7702\n",
      "[Iter 07700] ⏱ Time: 12157.4s | Avg Train Loss: 4.9254\n",
      "📉 Iter 7700: Val Loss = 5.7586\n",
      "[Iter 07750] ⏱ Time: 12263.3s | Avg Train Loss: 4.8544\n",
      "📉 Iter 7750: Val Loss = 5.7827\n",
      "[Iter 07800] ⏱ Time: 12369.9s | Avg Train Loss: 4.9357\n",
      "📉 Iter 7800: Val Loss = 5.7623\n",
      "[Iter 07850] ⏱ Time: 12477.2s | Avg Train Loss: 4.9180\n",
      "📉 Iter 7850: Val Loss = 5.7442\n",
      "[Iter 07900] ⏱ Time: 12582.4s | Avg Train Loss: 4.8988\n",
      "📉 Iter 7900: Val Loss = 5.7625\n",
      "[Iter 07950] ⏱ Time: 12687.2s | Avg Train Loss: 4.9136\n",
      "📉 Iter 7950: Val Loss = 5.7542\n",
      "[Iter 08000] ⏱ Time: 12792.0s | Avg Train Loss: 4.8649\n",
      "📉 Iter 8000: Val Loss = 5.7584\n",
      "[Iter 08050] ⏱ Time: 12898.5s | Avg Train Loss: 4.8919\n",
      "📉 Iter 8050: Val Loss = 5.7432\n",
      "[Iter 08100] ⏱ Time: 12982.6s | Avg Train Loss: 4.8938\n",
      "📉 Iter 8100: Val Loss = 5.7486\n",
      "[Iter 08150] ⏱ Time: 13083.5s | Avg Train Loss: 4.8719\n",
      "📉 Iter 8150: Val Loss = 5.7522\n",
      "[Iter 08200] ⏱ Time: 13190.3s | Avg Train Loss: 4.8779\n",
      "📉 Iter 8200: Val Loss = 5.7583\n",
      "[Iter 08250] ⏱ Time: 13295.7s | Avg Train Loss: 4.8404\n",
      "📉 Iter 8250: Val Loss = 5.7367\n",
      "[Iter 08300] ⏱ Time: 13401.1s | Avg Train Loss: 4.8594\n",
      "📉 Iter 8300: Val Loss = 5.7443\n",
      "[Iter 08350] ⏱ Time: 13508.7s | Avg Train Loss: 4.8507\n",
      "📉 Iter 8350: Val Loss = 5.7460\n",
      "[Iter 08400] ⏱ Time: 13607.8s | Avg Train Loss: 4.8332\n",
      "📉 Iter 8400: Val Loss = 5.7485\n",
      "[Iter 08450] ⏱ Time: 13708.5s | Avg Train Loss: 4.8627\n",
      "📉 Iter 8450: Val Loss = 5.7414\n",
      "[Iter 08500] ⏱ Time: 13808.9s | Avg Train Loss: 4.8441\n",
      "📉 Iter 8500: Val Loss = 5.7394\n",
      "[Iter 08550] ⏱ Time: 13867.2s | Avg Train Loss: 4.8642\n",
      "📉 Iter 8550: Val Loss = 5.7309\n",
      "[Iter 08600] ⏱ Time: 13905.7s | Avg Train Loss: 4.8269\n",
      "📉 Iter 8600: Val Loss = 5.7417\n",
      "[Iter 08650] ⏱ Time: 13946.3s | Avg Train Loss: 4.8573\n",
      "📉 Iter 8650: Val Loss = 5.7463\n",
      "[Iter 08700] ⏱ Time: 13985.5s | Avg Train Loss: 4.8359\n",
      "📉 Iter 8700: Val Loss = 5.7304\n",
      "[Iter 08750] ⏱ Time: 14024.8s | Avg Train Loss: 4.8265\n",
      "📉 Iter 8750: Val Loss = 5.7201\n",
      "[Iter 08800] ⏱ Time: 14064.4s | Avg Train Loss: 4.8406\n",
      "📉 Iter 8800: Val Loss = 5.7343\n",
      "[Iter 08850] ⏱ Time: 14103.9s | Avg Train Loss: 4.8242\n",
      "📉 Iter 8850: Val Loss = 5.7410\n",
      "[Iter 08900] ⏱ Time: 14143.6s | Avg Train Loss: 4.8222\n",
      "📉 Iter 8900: Val Loss = 5.7394\n",
      "[Iter 08950] ⏱ Time: 14183.3s | Avg Train Loss: 4.7865\n",
      "📉 Iter 8950: Val Loss = 5.7404\n",
      "[Iter 09000] ⏱ Time: 14223.2s | Avg Train Loss: 4.8233\n",
      "📉 Iter 9000: Val Loss = 5.7417\n",
      "[Iter 09050] ⏱ Time: 14263.2s | Avg Train Loss: 4.8262\n",
      "📉 Iter 9050: Val Loss = 5.7348\n",
      "[Iter 09100] ⏱ Time: 14303.5s | Avg Train Loss: 4.8348\n",
      "📉 Iter 9100: Val Loss = 5.7378\n",
      "[Iter 09150] ⏱ Time: 14343.4s | Avg Train Loss: 4.8363\n",
      "📉 Iter 9150: Val Loss = 5.7304\n",
      "[Iter 09200] ⏱ Time: 14383.2s | Avg Train Loss: 4.8254\n",
      "📉 Iter 9200: Val Loss = 5.7330\n",
      "[Iter 09250] ⏱ Time: 14423.2s | Avg Train Loss: 4.7988\n",
      "📉 Iter 9250: Val Loss = 5.7387\n",
      "[Iter 09300] ⏱ Time: 14462.5s | Avg Train Loss: 4.8042\n",
      "📉 Iter 9300: Val Loss = 5.7359\n",
      "[Iter 09350] ⏱ Time: 14502.6s | Avg Train Loss: 4.7724\n",
      "📉 Iter 9350: Val Loss = 5.7264\n",
      "[Iter 09400] ⏱ Time: 14542.4s | Avg Train Loss: 4.7831\n",
      "📉 Iter 9400: Val Loss = 5.7288\n",
      "[Iter 09450] ⏱ Time: 14582.5s | Avg Train Loss: 4.7911\n",
      "📉 Iter 9450: Val Loss = 5.7365\n",
      "[Iter 09500] ⏱ Time: 14622.2s | Avg Train Loss: 4.7914\n",
      "📉 Iter 9500: Val Loss = 5.7332\n",
      "[Iter 09550] ⏱ Time: 14663.5s | Avg Train Loss: 4.7740\n",
      "📉 Iter 9550: Val Loss = 5.7437\n",
      "[Iter 09600] ⏱ Time: 14703.4s | Avg Train Loss: 4.7850\n",
      "📉 Iter 9600: Val Loss = 5.7323\n",
      "[Iter 09650] ⏱ Time: 14743.9s | Avg Train Loss: 4.7664\n",
      "📉 Iter 9650: Val Loss = 5.7348\n",
      "[Iter 09700] ⏱ Time: 14783.7s | Avg Train Loss: 4.7559\n",
      "📉 Iter 9700: Val Loss = 5.7360\n",
      "[Iter 09750] ⏱ Time: 14825.7s | Avg Train Loss: 4.7776\n",
      "📉 Iter 9750: Val Loss = 5.7371\n",
      "[Iter 09800] ⏱ Time: 14865.4s | Avg Train Loss: 4.7673\n",
      "📉 Iter 9800: Val Loss = 5.7311\n",
      "[Iter 09850] ⏱ Time: 14905.0s | Avg Train Loss: 4.7566\n",
      "📉 Iter 9850: Val Loss = 5.7296\n",
      "[Iter 09900] ⏱ Time: 14945.9s | Avg Train Loss: 4.7427\n",
      "📉 Iter 9900: Val Loss = 5.7359\n",
      "[Iter 09950] ⏱ Time: 14985.8s | Avg Train Loss: 4.7750\n",
      "📉 Iter 9950: Val Loss = 5.7397\n",
      "[Iter 10000] ⏱ Time: 15025.7s | Avg Train Loss: 4.7659\n",
      "📉 Iter 10000: Val Loss = 5.7358\n",
      "📁 Training log saved to Sequence-to-Sequence model without attention_Mild_Extensions_2.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Instantiate Encoder and Decoder for Mild_Extensions_1 ===\n",
    "# Encoder processes the input (ingredients), and the Decoder generates the output (recipe)\n",
    "encoder_Mild_Extensions_1 = EncoderRNN(\n",
    "    input_vocab_size=len(input_vocab),   # Size of input vocabulary\n",
    "    emb_dim=EMB_DIM,                     # Embedding dimension\n",
    "    hidden_dim=HIDDEN_DIM                # Hidden layer size\n",
    ").to(device)\n",
    "\n",
    "decoder_Mild_Extensions_1 = DecoderRNN(\n",
    "    output_vocab_size=len(output_vocab), # Size of output vocabulary\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM\n",
    ").to(device)\n",
    "\n",
    "# === Define the baseline Seq2Seq model (without attention) ===\n",
    "model_Mild_Extensions_1 = Seq2Seq(\n",
    "    encoder=encoder_Mild_Extensions_1,\n",
    "    decoder=decoder_Mild_Extensions_1,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# === Define optimizer and loss function ===\n",
    "# Adam optimizer with small learning rate; loss function ignores <PAD> tokens\n",
    "optimizer_Mild_Extensions_1 = torch.optim.Adam(\n",
    "    model_Mild_Extensions_1.parameters(), lr=0.00005, weight_decay=1e-5\n",
    ")\n",
    "\n",
    "criterion_Mild_Extensions_1 = nn.CrossEntropyLoss(\n",
    "    ignore_index=output_vocab[PAD_TOKEN]\n",
    ")\n",
    "\n",
    "# === Train the model using the custom DataLoader and collate function ===\n",
    "# This training loop tracks performance and logs results to file and plot\n",
    "train_model(\n",
    "    model=model_Mild_Extensions_1,\n",
    "    train_loader=train_loader_Mild_Extensions_1,\n",
    "    val_loader=dev_loader_Mild_Extensions_1,\n",
    "    optimizer=optimizer_Mild_Extensions_1,\n",
    "    criterion=criterion_Mild_Extensions_1,\n",
    "    output_vocab=output_vocab_Mild_Extensions_1,\n",
    "    device=device,\n",
    "    num_iters=num_iters,\n",
    "    print_every=print_every,\n",
    "    plot_every=plot_every,\n",
    "    teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
    "    log_filename=\"Sequence-to-Sequence model without attention_Mild_Extensions_1\",\n",
    "    plot_title=\"Sequence-to-Sequence model without attention_Mild_Extensions_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92926ac",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84914364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Baseline predictions saved to Mild_Extensions_2_output.csv\n"
     ]
    }
   ],
   "source": [
    "model_word2vec.eval()\n",
    "baseline_Mild_Extensions_2_preds= []\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    ingredients_word2vec = test_df.iloc[i]['Ingredients']\n",
    "\n",
    "    # 🔁 Use same preprocessing as training\n",
    "    tokens_word2vec = preprocess_text(ingredients_word2vec)\n",
    "    input_ids_word2vec = [input_vocab.get(tok, input_vocab[UNK_TOKEN]) for tok in tokens_word2vec]\n",
    "    input_ids_word2vec = input_ids_word2vec[:MAX_INGREDIENT_LEN] + [input_vocab[PAD_TOKEN]] * max(0, MAX_INGREDIENT_LEN - len(input_ids_word2vec))\n",
    "\n",
    "    src_tensor_word2vec = torch.tensor(input_ids_word2vec, dtype=torch.long, device=device)\n",
    "\n",
    "    # 🧠 Generate recipe\n",
    "    generated_recipe_word2vec = generate_recipe(model_word2vec, src_tensor_word2vec, output_vocab, device, max_len = MAX_RECIPE_LEN, use_attention=False)\n",
    "    baseline_Mild_Extensions_2_preds.append(generated_recipe_word2vec)\n",
    "\n",
    "# 🗂️ Add predictions to test DataFrame\n",
    "test_df['Recipe - Mild_Extensions_2'] = baseline_Mild_Extensions_2_preds\n",
    "\n",
    "# ✅ Choose columns\n",
    "columns_to_save_Mild_Extensions_2 = ['Ingredients', 'Recipe - Mild_Extensions_2']\n",
    "if 'Recipe' in test_df.columns:\n",
    "    columns_to_save_Mild_Extensions_2.insert(1, 'Recipe')\n",
    "\n",
    "# 💾 Save to timestamped CSV\n",
    "filename_Mild_Extensions_2 = \"Mild_Extensions_2_output.csv\"\n",
    "test_df[columns_to_save_Mild_Extensions_2].to_csv(filename_Mild_Extensions_2, index=False)\n",
    "print(f\"✅ Baseline predictions saved to {filename_Mild_Extensions_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda2cca",
   "metadata": {},
   "source": [
    "#### Evalating the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f1844e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1081/1081 [00:36<00:00, 29.92it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc177d5d3f924a91b013513260790b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nidhi\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nidhi\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d85bae0e05846088c4c88557d845092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b425cdaa204d3b831751f2ea6784a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d31fc7aa19e4287af119f9a7a36c38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd159850b7042adbc15f8e7589b725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910d35af78fc4fb4b6b3282d985da47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Appended results to Assignment_2_evaluation_results_iter2000.csv\n",
      "                           Model      BLEU    METEOR  BERTScore\n",
      "0  Seq2Seq-RNN_Mild_Extensions_2  0.020084  0.121493    0.82098\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test_loader (or dev_loader if preferred)\n",
    "metrics_Mild_Extensions_2 = evaluate_model(model_word2vec, test_loader, input_vocab, output_vocab, device, model_name=\"Seq2Seq-RNN_Mild_Extensions_2\")\n",
    "\n",
    "# Step 2: Load existing results if the file exists\n",
    "results_file = \"Assignment_2_evaluation_results_iter2000.csv\"\n",
    "if os.path.exists(results_file):\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([metrics_Mild_Extensions_2])], ignore_index=True)\n",
    "else:\n",
    "    results_df = pd.DataFrame([metrics_Mild_Extensions_2])\n",
    "\n",
    "# Step 3: Save back to the same file\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"✅ Appended results to {results_file}\")\n",
    "print(results_df.tail(1))  # show just the latest entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef978b",
   "metadata": {},
   "source": [
    "### Toy Input texts (gold and predicted recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32cbaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_recipe_from_input(model, input_text, input_vocab, output_vocab, device,\n",
    "                              preprocess_fn=preprocess_text,\n",
    "                              max_input_len=MAX_INGREDIENT_LEN,\n",
    "                              max_output_len=MAX_RECIPE_LEN,\n",
    "                              use_attention=False):\n",
    "    \"\"\"\n",
    "    Generate a recipe from an input ingredient list using the provided model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Seq2Seq model.\n",
    "        input_text: Raw ingredient string.\n",
    "        input_vocab: Dictionary mapping input tokens to indices.\n",
    "        output_vocab: Dictionary mapping output indices to tokens.\n",
    "        device: Device to run the model on (CPU or GPU).\n",
    "        preprocess_fn: Preprocessing function used during training.\n",
    "        max_input_len: Maximum length of input sequence.\n",
    "        max_output_len: Maximum length of output recipe.\n",
    "        use_attention: Whether the model uses attention.\n",
    "\n",
    "    Returns:\n",
    "        Generated recipe string.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = preprocess_fn(input_text)\n",
    "    input_ids = [input_vocab.get(tok, input_vocab[UNK_TOKEN]) for tok in tokens]\n",
    "    input_ids = input_ids[:max_input_len] + [input_vocab[PAD_TOKEN]] * max(0, max_input_len - len(input_ids))\n",
    "\n",
    "    src_tensor = torch.tensor(input_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_recipe = generate_recipe(model, src_tensor, output_vocab, device,\n",
    "                                           max_len=max_output_len,\n",
    "                                           use_attention=use_attention)\n",
    "    return generated_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b76d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Generated Recipe baseline1:\n",
      " mix ingredient together pour hot water add ginger ale\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "user_input = \"sugar, lemon juice,  water,  orange juice, strawberries, icecream\"\n",
    "\n",
    "# Change model_baseline1 to your actual model\n",
    "output_recipe_baseline1 = predict_recipe_from_input(\n",
    "    model=model_word2vec, \n",
    "    input_text=user_input,\n",
    "    input_vocab=input_vocab,\n",
    "    output_vocab=output_vocab,\n",
    "    device=device,\n",
    "    use_attention=False  # or True if the model uses attention\n",
    ")\n",
    "print(\"🧾 Generated Recipe baseline1:\\n\", output_recipe_baseline1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "152c5b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Generated Recipe baseline1:\n",
      " mix cream cheese sugar milk add milk add milk add vanilla fold cool whip pour pie shell chill hour\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "user_input = \"8 oz philadelphia cream cheese, 14 oz can sweetened condensed milk, 1 ts vanilla, 1/3 c  lemon juice, 48 oz canned cherries, 8 inch graham cracker,  pie crusts\"\n",
    "\n",
    "# Change model_baseline1 to your actual model\n",
    "output_recipe_baseline1 = predict_recipe_from_input(\n",
    "    model=model_word2vec, \n",
    "    input_text=user_input,\n",
    "    input_vocab=input_vocab,\n",
    "    output_vocab=output_vocab,\n",
    "    device=device,\n",
    "    use_attention=False  # or True if the model uses attention\n",
    ")\n",
    "print(\"🧾 Generated Recipe baseline1:\\n\", output_recipe_baseline1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
